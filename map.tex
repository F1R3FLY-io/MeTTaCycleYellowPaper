\documentclass{article}
\usepackage{amsmath, amssymb, hyperref}
\usepackage{bussproofs}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage {mathpartir}
\usepackage[T1]{fontenc}
%% \usepackage[utf8x]{inputenc}

\include{map.local} 

\newcommand{\mycomment}[1]{}
\newcommand{\MC}{\mathsf{MeTTaCycle}}
\newcommand{\FD}{\mathsf{F1R3Dr1v3}}
\newcommand{\FF}{\mathsf{F1R3FLY.io}}
\newcommand{\RS}{\mathsf{RSpace}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code for SenaryInfC (six premises)
% Define preparation for six hypotheses
\def\prepSenary{%
    \ifnum\theLevel<6
        \errmessage{Hypotheses missing!}
    \fi%
    % Store the rightmost (6th) hypothesis
    \edef\rrrrcurBox{\thecur{myBox}}%
    \edef\rrrrcurScoreStart{\thecur{myScoreStart}}%
    \edef\rrrrcurCenter{\thecur{myCenter}}%
    \edef\rrrrcurScoreEnd{\thecur{myScoreEnd}}%
    \advance\theLevel by -1%
    % Store the 5th hypothesis
    \edef\rrrcurBox{\thecur{myBox}}%
    \edef\rrrcurScoreStart{\thecur{myScoreStart}}%
    \edef\rrrcurCenter{\thecur{myCenter}}%
    \edef\rrrcurScoreEnd{\thecur{myScoreEnd}}%
    \advance\theLevel by -1%
    % Store the 4th hypothesis
    \edef\rrcurBox{\thecur{myBox}}%
    \edef\rrcurScoreStart{\thecur{myScoreStart}}%
    \edef\rrcurCenter{\thecur{myCenter}}%
    \edef\rrcurScoreEnd{\thecur{myScoreEnd}}%
    \advance\theLevel by -1%
    % Store the 3rd hypothesis
    \edef\rcurBox{\thecur{myBox}}%
    \edef\rcurScoreStart{\thecur{myScoreStart}}%
    \edef\rcurCenter{\thecur{myCenter}}%
    \edef\rcurScoreEnd{\thecur{myScoreEnd}}%
    \advance\theLevel by -1%
    % Store the 2nd hypothesis
    \edef\ccurBox{\thecur{myBox}}%
    \edef\ccurScoreStart{\thecur{myScoreStart}}%
    \edef\ccurCenter{\thecur{myCenter}}%
    \edef\ccurScoreEnd{\thecur{myScoreEnd}}%
    \advance\theLevel by -1%
    % Store the leftmost (1st) hypothesis
    \edef\lcurBox{\thecur{myBox}}%
    \edef\lcurScoreStart{\thecur{myScoreStart}}%
    \edef\lcurCenter{\thecur{myCenter}}%
    \edef\lcurScoreEnd{\thecur{myScoreEnd}}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Define SenaryInf (with explicit conclusion)
\def\SenaryInf$#1\fCenter#2${%
    \prepSenary%
    \buildConclusion{#1}{#2}%
    \joinSenary%
    \resetInferenceDefaults%
    \ignorespaces%
}

% Define SenaryInfC (with conclusion given as argument)
\def\SenaryInfC#1{%
    \prepSenary%
    \buildConclusionC{#1}%
    \joinSenary%
    \resetInferenceDefaults%
    \ignorespaces%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Define how to join the six hypotheses into a single box
\def\joinSenary{%
    % Use the hypothesis separation as an hbox
    \setbox\myBoxA=\hbox{\theHypSeparation}%
    % Compute total horizontal extent:
    \lcurScoreEnd=\rrrrcurScoreEnd%
    \advance\lcurScoreEnd by\wd\rrrcurBox%
    \advance\lcurScoreEnd by\wd\rrcurBox%
    \advance\lcurScoreEnd by\wd\rcurBox%
    \advance\lcurScoreEnd by\wd\ccurBox%
    \advance\lcurScoreEnd by\wd\lcurBox%
    \advance\lcurScoreEnd by 5\wd\myBoxA%
    \displace=\lcurScoreEnd%
    \advance\displace by -\lcurScoreStart%
    \lcurCenter=.5\displace%
    \advance\lcurCenter by \lcurScoreStart%
    \ifx\rootAtBottomFlag\myTrue%
        \setbox\lcurBox=%
            \hbox{%
                \box\lcurBox\unhcopy\myBoxA%
                \box\ccurBox\unhcopy\myBoxA%
                \box\rcurBox\unhcopy\myBoxA%
                \box\rrcurBox\unhcopy\myBoxA%
                \box\rrrcurBox\unhcopy\myBoxA%
                \box\rrrrcurBox%
            }%
    \else%
        \htLbox = \ht\lcurBox%
        \htAbox = \ht\myBoxA%
        \htCbox = \ht\ccurBox%
        \htRbox = \ht\rcurBox%
        \htRRbox = \ht\rrcurBox%
        \htRRRbox = \ht\rrrcurBox%
        \htRRRRbox = \ht\rrrrcurBox%
        \setbox\lcurBox=%
            \hbox{%
                \lower\htLbox\box\lcurBox%
                \lower\htAbox\copy\myBoxA%
                \lower\htCbox\box\ccurBox%
                \lower\htAbox\copy\myBoxA%
                \lower\htRbox\box\rcurBox%
                \lower\htAbox\copy\myBoxA%
                \lower\htRRbox\box\rrcurBox%
                \lower\htAbox\copy\myBoxA%
                \lower\htRRRbox\box\rrrcurBox%
                \lower\htAbox\copy\myBoxA%
                \lower\htRRRRbox\box\rrrrcurBox%
            }%
    \fi%
    % Adjust the center of the combined box.
    \displace=\newCenter%
    \advance\displace by -0.5\newScoreStart%
    \advance\displace by -0.5\newScoreEnd%
    \advance\lcurCenter by \displace%
    % Save the assembled box and scores.
    \edef\curBox{\lcurBox}%
    \edef\curScoreStart{\lcurScoreStart}%
    \edef\curScoreEnd{\lcurScoreEnd}%
    \edef\curCenter{\lcurCenter}%
    \joinUnary%
}

\title{$\MC$ Architecture Proposal}
\author{}
\date{}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

$\MC$ is the codename for a new decentralized AI platform.  The
network will have agents, both human and AI, who can share digital
assets of all kinds, attenuate and delegate access to those assets,
and make powerful queries about each other's content.

We anticipate at least two interface points between humans and the
network; both of these will be based on existing user interfaces with
which users are already familiar.  The first is through the operating
system, where $\FD$ will be mounted into the filesystem.  The second
will be based on existing open source social networking software,
currently $\mathsf{BlueSky}$ and $\mathsf{Mastodon}$.

$\MC$, as its name suggests, will feature a version of the
$\mathsf{MeTTa}$ programming language.  The driving concept for
$\mathsf{MeTTa}$ is the idea that intelligence opportunistically
creates theories of computation to suit the domain in which the
intelligence is operating.  $\MC$ intends to connect many theories
operating in many domains, and to enable reasoning about these
interactions.

\subsection{The $\mathsf{MeTTa}$ Kernel}

SingularityNet hopes that $\mathsf{MeTTa}$ will be a language in which
one can describe a theory of computation and get an efficient
interpreter that is suited for smart contracts on a
blockchain~\cite{GoertzelMeredith2024}.

Without reproducing much of $\mathsf{MeTTa}$'s internals explicitly in
the theory, the alpha version of $\mathsf{MeTTa}$ has limitations with
respect to the ultimate goals for the language. Currently, the
alpha version cannot model different evaluation strategies for
lambda calculus, cannot model the semantics of the $\pi$ calculus and
other mobile process calculi, and has several other problems
preventing it from living up to this vision.  We propose an
architecture based on category theoretical concepts that will enable the next version of $\mathsf{MeTTa}$ to:
\begin{itemize}
    \item specify the semantics of any discrete theory of computation
    \item generate an efficient interpreter for that theory
    \item generate a sound type system customized to that theory
    \item support concurrency
    \item support transactions
\end{itemize}
and more.

Section \ref{Theories of computation} attempts to express some
theories of computation in the alpha version of $\mathsf{MeTTa}$ to
illustrate some of its limitations.  Section \ref{A way forward}
describes graph-structured lambda theories (GSLTs), which lifts these
expressability limitations.  Section \ref{Generating an interpreter
  and a type system} shows how from a GSLT we can generate an
efficient interpreter and a sound type system.  The resulting compiled
language is concurrent and transactional. Section \ref{Queries}
returns to the need to query a fact database and examines the kinds of
interesting query one can express in this system.

\subsection{Decentralization}

Were the deployment of an execution environment for $\mathsf{MeTTa}$
SingularityNet's only goal the $\MC$ architecture would comprise the
elements above. However, SingularityNet's goals include a
decentralized framework for AI, AGI, and ASI. As such, the
architecture must also address a security model for deployment of
execution outside of the protective womb of a firewall. Further, it
must embrace integration with a number of third party
services. Despite these additional constraints and their corresponding
remedies, $\MC$ must also be accessible and not hidden behind arcane
or cryptic command line interfaces.

To achieve the former we adopt a capabilities-driven tokenization
model. Access to compute and storage are mediated by object
capabilities amplified by tokens. See the \ref{Security model} for
details. To achieve the latter we provide integration of $\MC$ with
the file systems for the major operating systems; and, with major
social media platforms. See \ref{Deployment and integration}.

Finally, to achieve decentralization the $\MC$ architecture needs to
include fault-tolerance balanced against the need to replicate data
everywhere. It achieves this by delivering a sharded, heterogeneous
consensus mechanism. Each shard represents a unit of fault-tolerance
governed by a consensus mechanism. Shards may be organized into a
tree. Parent shards delegate to children when transactions only
involve resources confined to a single child shard, but provide a
governing consensus for transactions that implicate resources spanning
sibling shards.

\section{Market requirements}

\subsection{Security versus scaling and throughput}
The demand for decentralized digital asset management systems is on
the rise. By digital assets we don’t merely mean “tokens” as used in
BTC or ETH. Rather, we mean digitally recorded and stored information
from medical records to code (such as is found in GitHub); and we
expand this scope to agentic services that act on the behalf of their
users. The $\MC$ architecture provides such a digital asset
management service that guarantees the security model of Byzantine
fault tolerant networks (BTC, ETH) with the throughput and scale of
crash fault tolerant networks (Google Drive, Dropbox). This makes it
dramatically more secure and useful than any of the incumbent
platforms.

While the crash fault tolerant cloud-based networks have scaled to
serve millions of concurrent active users, they have been plagued by
hacks like Snowflake, Crowdstrike, and many others. By contrast, the
Byzantine fault tolerant networks (such as BTC) have huge unclaimed
``bug bounties''. Billions of dollars of assets have been flowing
through the BTC network for almost two decades without a successful
hack that would allow such a claim to be made. Nevertheless, although
the latter networks benefit from using Byzantine fault tolerant
technology, they are in themselves unable to scale --- a significant
limitation. $\MC$ scales to match the throughput characteristics of the
crash fault tolerant networks (CFTNs) and does this while also
supplying the security of the Byzantine fault tolerant networks
(BFTNs).

Furthermore, one of the crucial features of the CFTNs is their
searchability. One of the most important features enabling the new
global economy is search. When everyone with an Internet connection
became able to search --- for jobs or employees, goods, services, news,
analysis, etc. --- the global digital market became a reality,
supplanting oil as the most valuable sector. $\MC$ brings the
searchability of the CFTNs to the BFTNs. This not only means that data
stored on $\MC$ networks can be searched. It also means it can be
fed to and integrated with emerging AI solutions. Specifically,
$\MC$’s smart contracting language is, by design, a transactional
query language that works across an entire $\MC$ network at
scale, on the one hand; and a state of the art AI language on the other.

Unlike the BFTNs where tokenization seems to be primarily to encourage
trading, the key role of tokenization in $\MC$ networks is
fractional security. A token is a key to a unit of compute or a unit
of storage. Imagine if, rather than handing over your Netflix password
to your family members, you gave them an allotment of tokens, each one
representing a prescribed number of viewing hours. That’s what
$\MC$ tokens represent: a very fine-grained security mechanism
that enables careful accounting and auditing of system use and asset
access.

Most enterprises need a scalable storage solution. Typical
implementations bolt the security and accounting models on the side
and are subject to hacks along both interfaces (security and
accounting). In $\MC$ these two have been seamlessly integrated and
baked into searchable, transactional storage.

\subsection{Data and agents on chain}
The real benefit to $\mathsf{AI}$-oriented applications is not merely
security. Consider, the current generation of $\mathsf{LLM}$s were
trained primarily on data stolen from the Internet. The next
generation of models will not be. Instead they will be trained on data
hard won and costing the companies who developed it billions of
dollars. These companies will not just hand over their data to
$\mathsf{OpenAI}$ or $\mathsf{Anthropic}$. Companies like
$\mathsf{Merck}$ or $\mathsf{Pfizer}$ or $\mathsf{Astrazeneca}$ are
also loathe to spend additional billions developing in-house models.

A decentralized digital asset management system capable of hosting
data and agents on chain solves these problems. It allows companies
who hold highly valuable data sets to tokenize and monetize their data
and companies, like $\mathsf{Grok}$ who have developed models as a
service, to bring their services to the chain. We view this as the
next high growth marketplace. Companies like $\mathsf{OpaqueSystems}$ share
our view, but not our technology or approach.

\subsection{Integration into the technological ecosystem}

In order for this approach to work $\MC$ needs seamless integration
into the technological ecosystem. For example, $\MC$ is already
mountable as a file system on all major desktop platforms. The chain
just looks like another folder. Users can use their existing file
browsers to interact with the chain, that is simply drag and drop
audio, video, and text files directly onto chain and from chain. From
their point of view $\MC$ looks like a fancy $\mathsf{DropBox}$ or
$\mathsf{GoogleDrive}$.

Beyond this, a dominant mode of interaction between users is social
media. People have been trained on $\mathsf{Twitter}$,
$\mathsf{Facebook}$, and the like for more than a decade. $\MC$
recognizes this fact of user behavior in the market place. Already the
platform is demonstrating integration with $\mathsf{BlueSky}$ so that
$\MC$ acts as a backbone between independent $\mathsf{BlueSky}$
instances.

The plan is to integrate wallet and agent features into a fork of
$\mathsf{BlueSky}$ and $\mathsf{Mastodon}$ shortly after. This opens
up a world of new user opportunities. Tokenization takes on entirely
new meaning in this setting with users being able to share access to
data sets, compute, and agents directly with each other as part of
social communication. Likewise, they can moderate and control access
to feed-delivered content via tokens. Finally, they can create,
launch, interact with, shut down, and \emph{share} agents directly
with each other.

\subsubsection{Interoperability with other networks}
With the proliferation of the BFTNs the question of interoperability
looms large, both in terms of interoperation of different instances of
the same network, and interoperation of different kinds of
networks. In the first case private instances of a blockchain may be
operating in different divisions of a single company and it becomes
useful or necessary to have these instances interoperate to serve use
cases that cross these divisions. A similar sort of scenario arises
when rolling out different versions of a single network. In the second
case user network interactions may span multiple networks, travel
being a prime example. One can easily imagine the airlines settle on
one network, while the hospitality providers settle on another. The
user doesn't actually care. They just want to book a trip including
airfare and hospitality.

On the one hand $\MC$ is designed to be compositional. Shards can be
stitched togher into larger networks. On the other hand, the
channel-based model of $\MC$ makes it possible to treat other networks
as if they were shards. Still $\MC$ has larger aims. As will be
discussed, the technology offered here makes it possible for $\MC$
nodes to participate as first class citizens in other networks. This
opens up a new world of possibilities for how to bring features, like
data on chain or agents on chain to other networks.

\section{High-performance $\mathsf{MeTTa}$ Kernel}

\subsection{Theories of computation}
\label{Theories of computation}

In this section, we attempt to implement a couple of theories of
computation in the alpha version of $\mathsf{MeTTa}$ and uncover
some missing features and limitations with the current implementation.

\subsubsection{The theory of lambda calculus}
$\mathsf{MeTTa}$ can model the operational semantics of certain
virtual machines using a pattern similar to those of platforms like K
Framework~\cite{kframework}.  Here's a formalization of the lambda
calculus in $\mathsf{MeTTa}$, with an evaluation strategy that reduces
terms everywhere except under a lambda:

\begin{verbatim}
; -- The theory of lambda calculus

(: T Sort)
(: App (-> T T T))
(: Lam (-> (-> T T) T))
(= (App (Lam $f) $v) ($f $v))   ; Beta reduction

; -- Named functions due to lack of lambda in MeTTa

(: ident (-> T T))
(= (ident $x) $x)

(: omega (-> T T))
(= (omega $x) (App $x $x))

(: Omega (-> T T))
(= (Omega $x) (App (Lam omega) (Lam omega)))

; -- Example reductions

!(App X (App (Lam ident) Y))    ; [(App X Y)]
!(Lam Omega)                    ; [(Lam Omega)]
;!(App (Lam omega) (Lam omega)) ; infinite loop
\end{verbatim}

The theory introduces \verb+T+ as a sort, essentially the generator in
the grammar for terms.  It then declares the term constructors:
\verb+App+ takes two terms and produces a term, while \verb+Lam+ takes
a function from terms to terms and produces a term.  By using a
function type, the theory can avoid modeling explicit substitution and
alpha equivalence as part of the grammar by bootstrapping \verb+Lam+
using the binders in $\mathsf{MeTTa}$ itself.  Finally, the theory
declares the beta rule.

Note that there's nothing in the theory corresponding to the rule
defining the evaluation strategy:

\[ \frac{{\tt T} \leadsto {\tt T'} \quad {\tt U} \leadsto {\tt U'}}{\mbox{\tt (App T U)} \leadsto \mbox{\tt (App T' U')}}. \]

The evaluation strategy is hardcoded into the $\mathsf{MeTTa}$
interpreter.  We could not, for instance, choose to reduce solely in
head position:

\[ \frac{{\tt T} \leadsto {\tt T'}}{\mbox{\tt (App T U)} \leadsto \mbox{\tt (App T' U)}} \]

or to reduce under a lambda without considerably more machinery.

\subsubsection{The theory of RHO calculus}

Unfortunately, the alpha version of $\mathsf{MeTTa}$ is also unable
to model the RHO calculus~\cite{MeredithRadestock2005} (a Reflective
Higher-Order pi calculus) in a straightforward way:

\begin{verbatim}
(: P Sort)
(: N Sort)
(: Zero P)
(: Par (-> P P P))
(: Send (-> N P P))
(: Recv (-> N (-> N P) P))
(: At (-> P N))
(: Run (-> N P))
(= (Par (Send $chan $proc) (Recv $chan $cont)) ($cont (At $proc)))
(= (At (Run $n)) $n)
(= (Run (At $p)) $p)

; -- Problem: no support for structural equivalence.
; -- RHO terms form a commutative monoid, but in MeTTa
; -- the commutativity rewrite causes an infinite loop.

; (= (Par $p1 $p2) (Par $p2 $p1)) ; commutative
(= (Par $p Zero) $p) ; unital
(= (Par ((Par $p1 $p2) $p3) (Par $p1 (Par $p2 $p3)))) ; assoc.

; -- Helpers

(: nil (-> N P))
(= (nil $n) Zero)
(: chan (-> N))
(= (chan) (At Zero))

; -- Problem: reduction under a Send.
; -- According to RHO semantics, the printed process below should
; -- not reduce, but MeTTa reduces it to (Send (At Zero) Zero).

(: Proc (-> P))
(= (Proc) (Par (Send (chan) Zero) (Recv (chan) nil)))
!(Send (chan) (Proc))
\end{verbatim}

\noindent $\mathsf{MeTTa}$ does well at modeling the sorts and term
constructors.  There are sorts for Processes and Names of channels,
five term constructors, and three rewrite rules.  But RHO calculus
terms (as in all pi calculi) should be considered only up to
structural congruence.  The term constructors form a commutative
monoid under \verb+Par+, where \verb+Zero+ is the unit.
$\mathsf{MeTTa}$ has no concept of structural congruence.

One could argue that detecting whether two words are in the same
equivalence class is, in general, undecidable, and any real
implementation would use something like a normal form or a hash table
to eliminate the detection problem.  The Rholang interpreter, in fact,
uses such a strategy.  But the specific implementation details should
not be part of the abstract description of the language.

RHO calculus---like all pi calculi---is a model of concurrent
processes, and therefore can have races.  When multiple \verb+Send+s
or \verb+Recv+s are competing on the same channel, RHO calculus makes
a single nondeterministic choice of the winner of the race.
$\mathsf{MeTTa}$ takes all paths, so even very simple RHO calculus
programs require exponential space and time to execute on the research
version of $\mathsf{MeTTa}$ interpreter.

Finally, RHO calculus also forbids reduction under a Send.  For example, the term 
\[\mbox{\tt(Send (chan) Proc)},\]
where
\[\mbox{\tt Proc = (Par (Send (chan) Zero) (Recv (chan) nil))}\]
should not reduce.  The process \verb+Proc+ will always reduce to \verb+Zero+ if it is permitted to do so; but under the RHO calculus semantics, \verb+Proc+ should be suspended until it is received and executed with the \verb+Run+ constructor.  This matters because when \verb+Proc+ is received, it could be executed in the context of another \verb+Recv+ that could interfere on \verb+(chan)+ and the two \verb+Recv+s would race to claim the \verb+Send+.

\subsubsection{Other theories}

Other features missing from the alpha version of $\mathsf{MeTTa}$ that impair its ability to implement theories of computation include:
\begin{itemize}
    \item The lack of a \verb+lambda+ grounded atom for defining functions inline.
    \item No way to force evaluation of a subexpression.
    \item No way to add facts to the database as the result of some computation.
    \item A single shared fact database.
    \item No way to have theories execute concurrently.
    \item No transactions.
    \item No proof of soundness of a type system.
    \item No automatic way to update a type system to include two theories of computation made to interact.
\end{itemize}

\subsection{A way forward}
\label{A way forward}

The mathematician William Lawvere's research underpins all approaches
to formalizing theories of computation, so we review his contributions
and more recent generalizations below.  In this section, we show how
to use these theories to generate a reduction graph for a theory of
computation.  The vertices of the graph are the possible states the
computation can be in, and the edges are the possible state
transitions.  The section culminates in the notion of an interactive
graph-structured lambda theory (GSLT), which provides a language for
expressing computational theories that includes the ability to express
reduction strategies.

From any of these theories, we can derive a sound type system.
Because it is derived rather than invented, it allows an intelligence
to have type systems for interacting theories of computation and to
rederive them when it develops a new theory for some domain.

\subsubsection{Lawvere's algebraic theories}

In his 1963 PhD thesis~\cite{Lawvere1963}, William Lawvere introduced
the concept of an ``algebraic theory''.  His interest was in
constructing categories of algebraic structures like monoids, groups,
rings, fields, and so on.  A presentation of an algebraic theory looks
very much like the $\mathsf{MeTTa}$ presentation of a theory of
computation.

A presentation of an algebraic theory consists of:
\begin{itemize}
    \item A {\em sort}, say $T.$
    \item A set of {\em function symbols} $f_i$, each with a finite {\em arity}.  If $f_i$ has arity $n > 1$, we write $f_i:T^n \to T,$ where $T = T^1$ and $1 = T^0.$
    \item A set of {\em equations} between terms generated by the function symbols and a set of free variables.
\end{itemize}
For example, here's a presentation of the algebraic theory of groups Th(Grp):
\begin{itemize}
    \item A sort $G.$
    \item A function symbol $m\colon G^2 \to G$ for the multiplication.
    \item A function symbol $e\colon 1 \to G$ for the identity.
    \item A function symbol $i\colon G \to G$ for the inverse.
    \item An equation $m(m(a,b),c) = m(a, m(b,c))$ for the associative law.
    \item Equations $m(a, e) = a$ and $m(e, a) = a$ for the unit laws.
    \item Equations $m(a, i(a)) = e$ and $m(i(a), a) = e$ for the inversion laws.
\end{itemize}
The theory itself is essentially\footnote{There's a little more to the definition of a theory, but it's not critical to this discussion.} the free category with finite products on the data above.

A model $M$ of the theory is a product-preserving functor from the the theory to the category Set.  It picks out:
\begin{itemize}
    \item a set $M(G)$ of elements of the group
    \item a multiplication function $M(m)\colon M(G)^2 \to M(G)$
    \item a nullary function $M(e): 1 \to M(G)$ that returns the identity element
    \item an inversion function $M(i): M(G) \to M(G)$
\end{itemize}
such that multiplication is associative, unital, and invertible.

Lawvere proved that when an algebraic gadget is a set equipped with some functions satisfying some equations, the category of gadgets and gadget homomorphisms is equivalent to the category of product-preserving functors from the theory of gadgets to Set and the natural transformations between them.  He also proved that the theory is the opposite of the category of finitely generated gadgets.

\subsubsection{Multi-sorted and graph-structured theories}

Trimble~\cite{Trimble2018} generalized Lawvere theories, which have a single sort, to multi-sorted theories.  This lets us add interactions between different types of gadget; for example, a group action has both a group and a set for it to act on.  But more importantly for this discussion, it adds the ability to talk about edges in a reduction graph.

We can add a sort (say, $R$ for rewrites or reductions), function symbols $s$ and $t$ for the source and target of the rewrite, and function symbols to generate the grammar of the reductions.  With this approach, we can present the theory of SKI combinators (one of the earliest theories of computation, from 1924) with the reduction strategy that we only reduce in the head of an application:
\begin{enumerate}
    \item A sort $T$ for terms.
    \item A sort $R$ for rewrites.
    \item Term function symbols $s, t\colon R \to T$ for the source and target of a rewrite.
    \item A term function symbol $(-\; -)\colon T^2 \to T$ for application.
    \item Term function symbols $S, K, I\colon 1 \to T$ for the combinators.
    \item A rewrite function symbol $\sigma\colon T^3 \to R$ for reducing $S$ in the head.
    \item A rewrite function symbol $\kappa\colon T^2 \to R$ for reducing $K$ in the head.
    \item A rewrite function symbol $\iota\colon T \to R$ for reducing $I$ in the head.
    \item A rewrite function symbol $\eta\colon R \times T \to R$ for nesting reductions in the head.
    \item Equations $s(\sigma(x, y, z)) = (((S\; x)\; y)\; z)$ and $t(\sigma(x, y, z)) = ((x\; z)\; (y\; z)).$
    \item Equations $s(\kappa(x, y)) = ((K\; x)\; y)$ and $t(\kappa(x, y)) = x.$
    \item Equations $s(\iota(x)) = (I\; x)$ and $t(\iota(x)) = x.$
    \item Equations $s(\eta(r, x)) = (s(r); x)$ and $t(\eta(x)) = (t(r)\; x).$
\end{enumerate}
Note that we have function symbols for generating two grammars, the term grammar (items 1, 3-5) and the rewrite grammar (items 2, 6-9).  The equations say what the source and target of each rewrite is.  Items 9 and 13 encode the reduction strategy of reducing only in the head of an application:
\[ \frac{t \stackrel{r}{\leadsto} t'}{(t\; x) \stackrel{\eta(r, x)}{\leadsto} (t'\; x)}. \]

We'll adopt syntactic sugar for specifying the source and target of a rewrite and declare that $\rho(\vec{x})\colon T(\vec{x}) \leadsto U(\vec{x})$ is sugar for the pair of equations $s(\rho(\vec{x})) = T(\vec{x})$ and $t(\rho(\vec{x})) = U(\vec{x}).$

\subsubsection{Graph-structured lambda theories}

The inputs to function symbols in algebraic theories can only have product types.  Lambda theories expand the possible inputs to have function types as well.  With graph-structured lambda theories (GSLTs), we can present the theory of lambda calculus with the reduction strategy that we only reduce in the head of an application:
\begin{enumerate}
    \item A sort $T$ for terms.
    \item A sort $R$ for rewrites.
    \item Term function symbols $s, t\colon R \to T$ for the source and target of a rewrite.
    \item A term function symbol $(-\; -)\colon T^2 \to T$ for application.
    \item A term function symbol $[-]\colon (T \to T) \to T$ for abstraction.
    \item A rewrite function symbol $\beta\colon (T \to T) \times T \to R$ for beta reduction.
    \item A rewrite function symbol $\eta\colon R \times T \to R$ for nesting reductions in the head.
    \item Equations $\beta(f, v)\colon ([f]\; v) \leadsto f(v).$
    \item Equations $\eta(r, x)\colon (s(r)\; x) \leadsto (t(r)\; x).$
\end{enumerate}
Items 5 and 8, like the $\mathsf{MeTTa}$ theory of lambda calculus above, use the built-in binders of the lambda theory to avoid the boilerplate of explicit substitution and alpha equivalence.  Items 7 and 9 encode the reduction strategy in the same way as the SKI calculus above.

We can also present the theory of RHO calculus as a GSLT:
\begin{enumerate}
    \item Sorts $P$ and $N$ for processes and names, respectively.
    \item A sort $R$ for rewrites.
    \item A process function symbol $0\colon 1 \to P$ for \verb+Zero+.
    \item A process function symbol $|\colon P^2 \to P$ for \verb+Par+.
    \item A process function symbol $!\colon N \times P \to P$ for \verb+Send+.
    \item A process function symbol $?\colon N \times (N \to P) \to P$ for \verb+Recv+.
    \item A name function symbol $@\colon P \to N$ for \verb+At+.
    \item A process function symbol $*\colon N \to P$ for \verb+Run+.
    \item A rewrite function symbol $\chi\colon N \times P \times (N \to P) \to R$ for the communication event.
    \item A rewrite function symbol $\pi\colon R \times R \to R$ for parallel execution.
    \item Equations $\chi(x, p, q)\colon x!(p)\;| \; x?(q) \leadsto q(p)$ for a send and a receive synchronizing on the name $x.$
    \item Equations $\pi_1(r_1, x)\colon s(r_1)\;|\;x \leadsto t(r_1)\; |\; x$ for reductions in a par context.
    \item Equations $\pi_2(r_1, r_2)\colon s(r_1)\;|\;s(r_2) \leadsto t(r_1)\; |\; t(r_2)$ for parallel reductions.
    \item An equation $p\; |\; q = q\; |\; p$ for commutativity of \verb+Par+.
    \item An equation $(p\; |\; q)\; |\; r = p\; |\; (q\; |\; r)$ for associativity of \verb+Par+.
    \item An equation $0 \; |\; p = p$ for \verb+Zero+ being the unit.
    \item Equations $*@p = p$ and $@*n = n$ for suspension and execution.
\end{enumerate}
Under these rules, reduction only happens (as it should) in the context of a \verb+Par+ at the top level (items 9-13), not in a send or receive.

\subsubsection{Interactive GSLTs}

In the SKI calculus and lambda calculi, reduction is triggered by combining a function and a value in an application context.  In pi calculi and the ambient calculus~\cite{CardelliGordon2000}, reduction is triggered by combining a send and a receive in par context.  In a Turing machine, reduction is triggered by being in a tape context (as opposed to a halting context).  In cellular automata, reduction is triggered by being in a neighborhood context (which is always the case).  Every theory of computation of which we're aware has a term context that triggers reduction. We can model reduction contexts by picking a binary function symbol to be the trigger for reduction; we call this symbol the {\em interaction} and denote it formally by $\odot;$ for example, in pi calculus, $\odot = |.$  

Modal logic is a kind of logic used to reason about possibility and necessity.  In the context of theories of computation, we're interested in identifying properties of terms, like, ``Does this term necessarily terminate?'' or ``Is it possible for a client of this smart contract to extract all the tokens?''  Once we've chosen the interaction, we can derive certain useful modal types.  

For example, consider the modal type $\langle (- \; A) \rangle B$ of lambda terms that when applied to a value of type $A$ possibly reduce to a value of type $B$ in one step.  This modal type corresponds precisely to the arrow type $A \to B.$  There is nothing explicit about the arrow type in the theory of the lambda calculus; it simply falls out of the theory by considering terms in the first slot of the interaction.

Similarly, consider the modal type $[ - \; |\; A ] B$ of RHO processes that when juxtaposed with a process of type $A$ necessarily reduce to a process of type $B$ in one step.  This modal type corresponds precisely to Caires' rely-guarantee modality $A \triangleright B$~\cite{Caires2007}.

These modal types will play an important role in the generated type system described in the next section.

\subsection{A GSLT for $\mathsf{MeTTa}$}
In the same way that there is a grammar for BNFC, there is a GSLT
$\mathsf{Th}(GSLT)$ for presenting finitely presentable GSLTs.  It has
no rewrites, because it's just describing the grammar of GSLTs.  We
can extend $\mathsf{Th}(GSLT)$ with a grammar for facts and queries,
with rewrites for processing those queries; see section \ref{Queries}
below for more details.

\subsubsection{MeTTaIL}

$\mathsf{MeTTaIL}$ takes the idea presented above of a self-describing
GSLT for describing GSLTs seriously, and provides a clean intermediate
language for representing theories in a way that is more
compiler-friendly than the alpha version of $\mathsf{MeTTa}$'s
design. That is, $\mathsf{MeTTaIL}$ files represent compilation units
in much the same way that a file in $\mathsf{Java}$ represents a
compilation unit.

Beyond this, $\mathsf{MeTTaIL}$ is the input language for the
algorithm for generating an interpreter and the algorithm for
generating a type system. The $\mathsf{GitHub}$ repo for the
self-describing spec for $\mathsf{MeTTaIL}$ is linked
$\href{https://github.com/F1R3FLY-io/MeTTa/blob/dev/GSLT/src/main/bnfc/metta_venus.cf}{here}$. To make this paper a little more self-contained we have included an appendix with a summary of the spec: \ref{Appendix A}. The repo contains as a test an example of a $\mathsf{MeTTaIL}$ specification for $\mathsf{rholang}$
which is linked
$\href{https://gist.github.com/leithaus/9cef95f3b703a406385e94cb6b1aab67}{here}$. Again, to help with some of the context switching, we have included this content as an appendix \ref{Appendix B}.

The $\mathsf{rholang}$ example is of particular note in the
decentralization space. $\mathsf{Rholang}$ is a smart contracting
language. We fully anticipate being able to provide $\mathsf{MeTTaIL}$
specs for other smart contracting languages including
$\mathsf{Solidity}$, as well as $\mathsf{Solana}$ and
$\mathsf{Cardano}$'s smart contracting languages. Taken together with
the fact that $\MC$ nodes are capable of operating in heterogeneous
consensus environments, in this way we aim to allow $\MC$ nodes to be
fully compliant nodes in other chains. However, these nodes will also
allow data on chain and agentic computation.

\subsection{Generating an interpreter and a type system}
\label{Generating an interpreter and a type system}

In this section we consider the problem of generating an efficient interpreter and a type system from a GSLT.

\subsubsection{Fine- and coarse-grained GSLTs}
In the RHO calculus, the terms form a commutative monoid.  However, there are $n!$ permutations of a term with $n-1$ parallel atomic processes; if we had to enumerate all the permutations until we found a term in the right form to reduce, it would be absurdly slow.  Even worse, the general problem of determining whether two words are in the same equivalence class is undecidable.  Actual implementations of process calculi will do things like replace variables with keys into a hash table, store all the receiving processes in buckets under their key, and then iterate over the sends looking for matches.  Therefore, when generating a vm and a type system from a GSLT, a language implementer will likely want to provide two GSLT presentations and a proof.

The first presentation will be of the fine-grained GSLT with all the optimizations and implementation details; it will only have equations pertaining to the source and target of rewrites and will have logic for things like reducing to a normal form or using a hash table.  We can derive a type system from such a GSLT, but it will likely be too complicated to be used by developers writing programs in the language.

The second will be a coarse-grained GSLT that abstracts away the implementation details and things like specific choices of normal form.  This is the GSLT that we feed to the algorithms above to generate a useful type system.  From an arbitrary GSLT we can automatically derive a ``native'' type system~\cite{WilliamsStay2021}---the internal language of the topos of presheaves on the theory---and from an interactive GSLT, we can derive a smaller but still very useful dependent type system similar to Barendregt's lambda cube~\cite{HypercubePaper}.  The modalities in the previous section play the role analogous to that of the dependent product in the lambda cube.

The proof should show that the fine-grained GSLT is a valid implementation of the coarse-grained one.  One way to do this is with a necessity-preserving functor between the free quivers on the reduction graphs.  Possibility is preserved by any functor between the quivers: given a possible rewrite from one term to another in the coarse-grained GSLT, there is a path of possible rewrites between the corresponding terms in the fine-grained GSLT.  Preserving necessity means that given a necessary rewrite from one term to another in the coarse-grained GSLT, there is a path of necessary rewrites between the corresponding terms in the fine-grained GSLT.

\subsubsection{Compilation to Rholang}
\label{comp_to_rholang}

Rholang is an extension of the RHO calculus that seamlessly combines synchronous datatypes with the best-studied approach to concurrency, the $\pi$-calculus, to provide an efficient, transactional knowledge store.  Given a finitely-presented fine-grained GSLT, we would like to produce a Rholang program that implements it.  The general strategy is to send the state of the system on a channel, then produce a sum of ``edge'' processes that implement rewrites out of the state.  Each edge process in the sum attempts to consume the current state and produce the edge's target state.

%% We want to point out that we are using a variant of rholang. Specifically, version 1.2 with unification. We can summarize explanatory text from metta-calculus paper and reference that.

\paragraph{Enumerating rewrites}
\label{enum_rewrites}

We can't produce a sum of all the edge processes out of a state simultaneously due to a finite amount of memory.  In an implementation of the lambda calculus with a reduction strategy that allows beta reduction anywhere within a term, it is easy to produce an exponential number of possible rewrites out of a term.  For example, suppose we have a bunch of applications of the identity combinator \verb+I = +$\lambda x.x$ to itself:

\begin{verbatim}
((I I) ((I I) ((I I) ((I I) ((I I) I)))))
\end{verbatim}

There are five different places in this lambda term where a beta reduction could occur, so there are $2^5 = 32$ different parallel reductions that could occur.

Even worse, consider the following modification of RHO calculus' Comm rule: \bigskip

\noindent ${\rm Comm}(x, K, Q_1, Q_2): {\rm Send}(x, Q_1) \;|\; {\rm For}(x, K) \leadsto Q_2.$ \bigskip

\noindent This says that any interacting pair of processes can evolve to {\em any other term}!  It is useless computationally, but demonstrates that in principle we can have an infinite number of rewrites out of a source term.

We can write an interpreter with a process that unfolds the sum dynamically.  At each step, the interpreter may choose to synchronize with of the existing edge processes in the sum or to recurse and generate another edge process.

\begin{verbatim}
Intepreter = EdgeGen(state, 0)

EdgeGen(state, n) =
    EdgeProcess(state, n) + EdgeGen(state, n + 1)
\end{verbatim}

This technique produces a single-threaded interpreter that includes in its enumeration parallel rewrites on parts of the state.  It suffices for an interpreter, but does not make good use of the massive parallelization that $\RS$ provides.

\paragraph{Adding parallelism}

A GSLT has a finite number of top-level and in-context rewrite constructors.  Top-level rewrites unify against a whole term, while in-context rewrites unify against part of a term and then recurse.  The recursion must terminate because the terms are of finite length.

However, the interpreter is not given rewrites, it's given a term that may be the source of a rewrite.  Matching the state agains the source of a rewrite constructor is a unification problem.  The implementation above needs to verify that the entire term is the source of a rewrite in order to add that edge process to the sum.  Rather than preverifiy and sum the resulting processes, we can delegate the verification to processes that are {\em potentially} edges out of the state and simply run them in parallel.  Any potential edge process that manages to verify that the current state really is the source of that edge can then send a message to a channel waiting for a winner.

RHO calculus only has public names, so if we tried this technique in RHO calculus, it would leave a lot of garbage laying around:

\begin{verbatim}
@Nil!(1) | @Nil!(2) | for (winner <- @Nil) stdout!(*winner)

    | 2 wins
    V

@Nil!(1) | stdout!(2)
\end{verbatim}

\noindent However, Rholang also allows private names, like those used by pi calculus.  If we use private names, the loser can get garbage collected because we're guaranteed that no other process can synchronize with it:

\begin{verbatim}
new x in { x!(1) | x!(2) | for (winner <- x) stdout!(*winner) }

    | 2 wins
    V

new x in { x!(1) | stdout!(2) }

    || x not free in stdout!(2)

new x in { x!(1) } | stdout!(2)

    || no synchronization possible

Nil | stdout!(2)

    || monoid laws

stdout!(2)
\end{verbatim}

This design pattern allows a much simpler interpreter that has one potential edge process for each rewrite constructor in the GSLT.  If an in-context rewrite constructor matches the state, the interpreter forks new processes that independently try to verify that the substructure is the source of a rewrite, then joins those processes and signals that it found an actual edge.

\paragraph{Detecting optimization opportunities}

There are often more opportunities for optimization, but they only apply in certian circumstances.  Detecting opportunities for optimization is a standard part of interpreter design, and will be important for a compilation pipeline targeting Rholang.  For example, in a programming language with functions and an eager reduction strategy, it doesn't make sense to decompose the state from the top level at each step; instead, subexpressions should be reduced completely before moving up in the syntax tree.  Standard heuristics can be brought to bear on the sets of rewrite constructors, and automated techniques exist for generalizing specific instances of optimizations to the most general applicable situation~\cite{Tate2010}.

\subsection{$\RS$}

The Rholang interpreter uses a very efficient data structure called $\RS$ for storing continuations and matching sends with receives.  To a first approximation, the Rholang interpreter is just a parser sitting on top of $\RS$.

In the simplest cases, the names on which processes are sending and receiving get hashed, and if there's a process of the opposite polarity waiting in the hash table, they can synchronize.  $\RS$ also allows processes to bind variables using patterns.  This lets programmers dispatch messages on the same channel to different processes based on the content of the messages, and is the basis of the design of the interpreters above.

\subsubsection{$\RS$ and $\mathsf{MORK}$}

$\mathsf{MORK}$ ($\mathsf{MeTTa}$ Optimal Reduction Kernel) is a powerful functional language for manipulating graph databases in memory. It represents sets of paths using a trie, which can be exponentially smaller than the original dataset.  It can also act on all the paths in a subtrie simultaneously, which can be exponentially faster than mapping over a data set.  $\mathsf{MORK}$ was designed with provable efficiency in mind: each operation provides complexity guarantees.  Using $\mathsf{MORK}$ lets a programmer make accurate predictions about the computational complexity of a query.

However, $\mathsf{MORK}$ does not have any transactional semantics or primitives for concurrency.  As such, $\RS$ and $\mathsf{MORK}$ are complementary languages, well-suited to each other.  Rholang 1.2 will embed $\mathsf{MORK}$ as a datatype in the same way that it embeds numbers, strings, lists, maps, and sets.  It will also use $\mathsf{MORK}$ for unification with patterns in receiving processes instead of merely binding a single variable.

\subsection{Queries}
\label{Queries}

There is a long tradition of associating types with predicates.  One of the earliest kind of query was whether a term halts or not.  This is, as Church and Turing showed, undecidable.  But the subsequent development of type systems for the lambda calculus focused on that question, and any well-typed program in any of the type systems of the lambda cube is guaranteed to halt.

Many modern languages also use predicates for types.  TypeScript, for example, includes a notion of narrowing the type of a value within \verb+if+ blocks whose condition tests some property of the value.  The native type theory mentioned above is the internal language of the topos of presheaves on the theory, and this internal language includes the ability to express predicates; but more generally, we can simply ask for witnesses of any particular type in a type system.  

Finding such witnesses is the subject of an enormous literature and is outside the scope of this paper, but restricted problems can be solved quickly (e.g. $\mathsf{MORK}$ handles structural type queries very quickly).  Certainly Prolog's backtracking approach is a brute-force way of finding witnesses, but there are many other ways, and an interpreter supporting queries should expose many different methods with clear complexity guarantees.

The automatically generated type systems above give us grammars in which to express queries in the form of an expanded GSLT including both the original operational semantics and new type-constructing function symbols. Since queries are about terms in a GSLT and the type system itelf is expressed as a GSLT, queries can produce new queries in which facts are added, subtracted, or transformed.

The notion of a theory of computation is very general; one could, for example, model a cell as a computing device where terms are cell states and rewrites involve molecules and cell signaling pathways.  Healthy and diseased cells become types expressible in the type system, and we have modal types corresponding to small molecules that in the context of a diseased cell lead to a healthy cell.

\section{Decentralizing $\mathsf{MeTTa}$}

Rholang is an object capapbilities platform.  On a single logical machine like a blockchain, private names created with the \verb+new+ operator are unforgeable\footnote{A capability is forgeable if you can create an instance of it given only bits. Object references in memory safe languages are unforgeable, since even if you have the memory address of an object, there's no way to turn it into an object reference.} bearer tokens for access to a process.  On an open network, the only things that can be transmitted are bits, so names are necessarily forgeable.  But they can be made unguessable, like the long random strings in API keys.

These names can be attenuated and delegated, and interposing processes can enable arbitrarily fine-grained security patterns~\cite{Stiegler2004}.  For example, Alice can make access to a process $P$ listening for messages on channel $p$ revocable by interposing another forwarder process $F$ that listens on two channels $f$ and $r$.  Alice shares revocable access to $P$ by sending Bob the name $f,$ but she keeps $r$ to herself.  $F$ forwards to $P$ on $p$ any messages that it receives on $f$ from Bob until $F$ receives a revocation trigger from Alice on $r$.  At that point, $F$ shuts down and Bob has no way to contact $P$.

$\MC$, however, will also provide metered access through the use of tokens.

\subsection{Tokenization and capabilities}
\label{Security model}

This tokenized security model is \emph{generated} from the operational
semantics. Here, for example, we apply the method to the operational
semantics outlined in the $\mathsf{MeTTa}$-calculus.

\begin{mathpar}
  \inferrule* [lab=security-tokens] {} {T \bc () \;\bm\; s \;\bm\; T:T}
  \and
  \inferrule* [lab=secured-processes] {} {S \bc \mathsf{\{}P\mathsf{\}}_{s} \;\bm\; T \;\bm\; S\mathsf{|}S} \\
  \and
  \inferrule* [lab=multi-party-sigs] {} {s \bc () \;\bm\; \mathsf{hash}( <\!signature\!> ) \;\bm\; s\mathsf{\&}s}
\end{mathpar}

where $\mathsf{\{}P\mathsf{\}}_{s}$ is a process signed by a digital signature.

\begin{mathpar}
  \inferrule* [lab=COMM-cosigned-par-external-sequential] {\sigma = \mathsf{unify}(t,u)} {\mathsf{\{}\mathsf{for}( t \sngllrarrow x )P\mathsf{\}}_{s_1} \;\mathsf{|}\; \mathsf{\{} x \mathsf{!}( u )\mathsf{;} Q \mathsf{\}}_{s_2}\;\mathsf{|}\; s_{1}\mathsf{\&}s_{2}:T \red \mathsf{\{}P\sigma\mathsf{|}Q\sigma\mathsf{\}}_{s_1\mathsf{\&}s_2} \;\mathsf{|}\; T} \\
  \and
  \inferrule* [lab=COMM-cosigned-par-external-concurrent] {\sigma = \mathsf{unify}(t,u)} {\mathsf{\{}\mathsf{for}( t \sngllrarrow x )P\mathsf{\}}_{s_1} \;\mathsf{|}\; \mathsf{\{} x \mathsf{!}( u )\mathsf{;} Q \mathsf{\}}_{s_2}\;\mathsf{|}\; s_{1}:T_{1}\;\mathsf{|}\;s_{2}:T_{1} \red \mathsf{\{}P\sigma\mathsf{|}Q\sigma\mathsf{\}}_{s_1\mathsf{\&}s_2} \;\mathsf{|}\; T_{1} \;\mathsf{|}\; T_{2}} \\
  \and
  \inferrule* [lab=COMM-signed] {P \red P'} {\mathsf{\{}P\mathsf{\}}_{s} \;\mathsf{|}\; s:T \red \mathsf{\{}P'\mathsf{\}}_{s} \;\mathsf{|}\; T} \\
  \and
  \inferrule* [lab=COMM-cosigned-par-internal] {P \red P'} {\mathsf{\{}P\mathsf{\}}_{s_1 \& s_2} \;\mathsf{|}\; s_{1}:T_{1} \;\mathsf{|}\; s_{2}:T_{2} \red \mathsf{\{}P'\mathsf{\}}_{s_1 \& s_2} \;\mathsf{|}\; T_{1}\;\mathsf{|}\;T_{2}}  
\end{mathpar}

The last two rules are easy to understand. The third rule says that a
computation housed entirely within the membrane of a signature
requires a token provided by the agent(s) who has (have) the signing
authority of that signature, while the fourth rule says that a
computation housed entirely within the membrane of a multi-party
signature may use the tokens of each individual signer.

The first two rules are about computation involving synchronization
across security boundaries. The first rule says that for a
transactional exchange of data where the receiving computation is
signed by one party and the sending computation is signed by another,
then a multi-sig token containing the signatures of both parties
suffices to allow the synchronization. Meanwhile the second rule says
that the same situation can progress by taking a token from the
receiving and a token from the sending parting. Note that the
resulting computation is now signed by both parties and to progress
requires tokens from both parties (see rules three and four).

\subsubsection{Unification and other ancillary costs}

More generally, the method described is an endofunctor on the category
of GSLTs. It is not the only such. For instance, it is worth noting
that this functor does not take into account the cost of unification,
which in some cases can be considerable. A basic approach is to demand
that unification return a cost along with the substitution. The cost
can be treated as a constant or a multiple of the cost. For example,

\begin{mathpar}
  \inferrule* [lab=COMM-cosigned-par-external-concurrent] {( \sigma, n ) = \mathsf{unify}(t,u)} {\mathsf{\{}\mathsf{for}( t \sngllrarrow x )P\mathsf{\}}_{s_1} \;\mathsf{|}\; \mathsf{\{} x \mathsf{!}( u )\mathsf{;} Q \mathsf{\}}_{s_2}\;\mathsf{|}\; \underbrace{s_{1} : \ldots : s_{1}}_{n} : T_{1}\;\mathsf{|}\;\underbrace{s_{2} : \ldots : s_{s}}_{n} : T_{1} \\ \red \mathsf{\{}P\sigma\mathsf{|}Q\sigma\mathsf{\}}_{s_1\mathsf{\&}s_2} \;\mathsf{|}\; T_{1} \;\mathsf{|}\; T_{2}} \\
\end{mathpar}

These techniques can be used liberally to treat all manner of
additional costs incurred in actual calculations on physical hardware.

\section{Fault-tolerance: embedding the kernel in a consensus mechanism}

% mention swappable consensus
One critical aspect of $\MC$ is that it makes virtually no commitment
to a particular consensus mechanism. Rather, consensus is viewed as a
tool for providing fault-tolerance over a community of nodes. As such,
$\MC$ provides many different consensus mechanisms out of the box and
will provide more as time goes on.

Fault-tolerance means different things in different use cases. For
example, high-speed trading where the transactions carry very small
data payloads can and should be supported by a different algorithm
than data-on-chain where the data payloads can be arbitrarily
large. Likewise, computing-at-the-edge use cases where nodes are
running on mobile devices need yet a different algorithm.

$\MC$ plans to support each of these use cases as part of its roadmap.

\subsubsection{Correct-by-construction Casper}

$\mathsf{CBC-Casper}$ was the result of a collaboration amongst
Vitalik Buterin, Vlad Zamfir, and Lucius Gregory Meredith. Amongst its
many innovations, it uses ideas from the games semantics of linear
logic to prevent equivocation. $\MC$ has production implementations in
both $\mathsf{Scala}$ and $\mathsf{Rust}$ of
$\mathsf{CBC-Casper}$. Its principle contemplated use case is
data-on-chain.

\subsubsection{Cassanova}

$\mathsf{Cassanova}$ was developed by Pyrofex with a focus on
high-speed trading.

\subsubsection{Cordial miners}

The cordial miners consensus algorithm was developed by Shapiro, et
with a focus on computing-at-the-edge.

\section{Deployment and integration}
\label{Deployment and integration}

To aid adoption and provide practical deployment $\MC$ comes with
integration to major throughways in the creation and dissemination of
digital assets: the OS and social media.

\subsection{Integration with the OS}

In the case of integration with the OS $\MC$ uses
$\mathsf{libFUSE}$ to mount a remote shard as if it is a local file
system. Effectively, $\MC$'s performance is on par with what
users normally experience with Dropbox or Google Drive. Using their
local file browser, with no modifications, they can drag and drop
assets onto a shard. This allows for users to share data with each
other over a decentralized network with the security of common BFTNs,
such as BTC or ETH. See the $\FD$ repo.

\begin{figure}
  \centering
  \includegraphics[scale=0.15]{MeTTaCycleArchitecture2025OSDeployment.pdf} \\
  \caption{OS/File system deployment}
  \label{MCArch2025OSDeployment}
\end{figure}

\subsection{Integration with social media}

In the case of integration with social media $\MC$ offers two
levels of integration. The first of these is importing and exporting
between local caches. For example, Zulip, $\mathsf{BlueSky}$, and
$\mathsf{Mastodon}$ all use Postgres as their local instance
storage. $\MC$ allows import and export between their Postgres
storage and the shard. Thus, $\MC$ becomes a bus between
different instances -- without modification of the social media
application stack.

The second of these is a tighter integration involving modifications
to the social media stack in order to add features made possible by
tokenization of capabilities and the integration of agentic AI.

\begin{figure}
  \centering
  \includegraphics[scale=0.15]{MeTTaCycleArchitecture2025SocialDeployment.pdf} \\
  \caption{Social deployment}
  \label{MCArch2025SocialDeployment}
\end{figure}

\section{Roadmap}

Please refer to the live roadmap planning document linked $\href{https://docs.google.com/spreadsheets/d/1XqL1_ludowA7YHjDkHuX9lQyY5-NBH-mKOMDr4SUQtc/edit?usp=sharing}{here}$.

\section{$\mathsf{MeTTaIL}$ and AI}

\subsection{Evolutionary programming: Mating Theories in Practice}

We describe an approach to evolutionary programming for GSLTs.  We
assume an interactive GSLT, say $I$. That is, $I$ enjoys a
hypothesis-free (or base) reduction rule of the form $K( P, E ) \leadsto K'(
P', E' )$.  And, for simplicity, we assume this is the only rewrite of
the theory. \footnote{We can detect this with a spatial type on
MeTTaIL theories.}

Here $K$ is some constructor of the theory, $P$ is a subtype of $Proc$
(the principal export of $I$), $E$ is also subtype of $Proc$. We use the
mnemonics $P$ and $E$ to remind us that $P$ is to be thought of as
"program" (aka the agent or subject of the state), and $E$ is to be
thought of as environment (the aspect of the state understood as
the context in which the program or agent is operating).

\paragraph{Examples}:
\begin{itemize}
\item $\lambda$-calculus:
  \begin{itemize}
  \item $\mathsf{App}( \mathsf{Abs}( [x]f ), arg ) \leadsto f[ arg / x ]$
  \item $K = \mathsf{App}$, $P = \mathsf{Abs}( [x]f )$, $E = arg$,
  \item $P' = f[ arg / x ]$, $E' = \epsilon$, $K' = \Box$  
  \end{itemize}
\item rho calculus:
  \begin{itemize}
    \item $\mathsf{Par}( \mathsf{Recv}( x, R ), \mathsf{Send}( x, Q ) ) \leadsto R( @Q )$
    \item $K = \mathsf{Par}$, $P = \mathsf{Recv}( x, [y]R )$, $E = \mathsf{Send}( x, Q )$,
    \item $P' = R[ @Q / y ]$, $E = 0$, $K' = \Box$    
  \end{itemize}
\item or
  \begin{itemize}
  \item $P = \mathsf{Send}( x, Q )$, $E = \mathsf{Recv}( x, R )$,
  \item $P' = 0, E' = R( @Q )$, etc
  \end{itemize}
\end{itemize}

Is there a cross-over operator between two interactive GSLTs, say $I$ and $J$?

Suppose that $Terms( I ) = Terms( J )$. For simplicity we will also
suppose that $Equations( I ) = Equations( J )$. However, we will
revisit this.  Suppose further that $base( I ) = F_s( F_{1_s}( P ),
F_{2_s}( E ) ) \leadsto F_t( F_{1_t}( P' ), F_{2_t}( E' ) )$ and $base( J ) = M_s(
M_{1_s}( P ), M_{2_s}( E ) ) \leadsto M_t( M_{1_t}( P' ), M_{2_t}( E' ) )$.  Then we
can define a collection of new theories, denoted $I \Join J$. To
specify this collection we define some useful auxilliary collections.

Ed note: arities have to match.

\begin{align}
  \mathsf{src}( I \Join J ) &= \{ \\
    & F_s( F_{1_s}( P ), F_{2_s}( E ) ),\\
    & M_s( M_{1_s}( P ), M_{2_s}( E ) ),\\
    & F_s( M_{1_s}( P ), F_{2_s}( E ) ),\\
    & M_s( F_{1_s}( P ), M_{2_s}( E ) ),\\
    & F_s( F_{1_s}( P ), M_{2_s}( E ) ),\\
    & M_s( M_{1_s}( P ), F_{2_s}( E ) ),\\
    & F_s( M_{1_s}( P ), M_{2_s}( E ) ),\\
    & M_s( F_{1_s}( P ), F_{2_s}( E ) )\\
  \}
\end{align}

\begin{align}
  \mathsf{trgt}( I \Join J ) &= \{ \\
    & F_t( F_{1_t}( P' ), F_{2_t}( E' ) ),\\
    & M_t( M_{1_t}( P' ), M_{2_t}( E' ) ),\\
    & F_t( M_{1_t}( P' ), F_{2_t}( E' ) ),\\
    & M_t( F_{1_t}( P' ), M_{2_t}( E' ) ),\\
    & F_t( F_{1_t}( P' ), M_{2_t}( E' ) ),\\
    & M_t( M_{1_t}( P' ), F_{2_t}( E' ) ),\\
    & F_t( M_{1_t}( P' ), M_{2_t}( E' ) ),\\
    & M_t( F_{1_t}( P' ), F_{2_t}( E' ) )\\
  \}
\end{align}

Finally, we can define

\begin{align}
  I \Join J \\
  & = \\
  & \mathsf{for}( src \leftarrow \mathsf{src}( I \Join J ); \; trgt \leftarrow \mathsf{trgt}( I \Join J ) ) \; \mathsf{yield}\; \{ \\
  & \mathsf{Theory}\; \mathsf{mkName}( src, trgt )() \{ \\
       & \quad \mathsf{Nil} \\
    & \quad \mathsf{Terms}\; \{ \mathsf{Terms}( I ) \} \\
    & \quad \mathsf{Equations}\; \{ \mathsf{Equations}( J ) \} \\
    & \quad \mathsf{Rewrites}\; \{ src \leadsto trgt \} \\
  \}
\end{align}

Notice that we could have applied the same technique to contextual rewrites,
as well as equations.

Now, we can describe a population, say $Pop$, (of theories) distributed over a
namespace, say $N$, according to a distribution, $D$.

\begin{align*}
  \mathsf{let}\; islands( Pop, N, D )\; &= \\
  & \mathsf{letrec}\; Locate( Pop, N, D ) &= \\
    &\quad \mathsf{foldLeft}( \\
      &\quad\quad Pop, \\
      &\quad\quad 0, \\
      &\quad\quad ( acc, e ) \;\mathsf{=>}\; \{ \\
      &\quad\quad\quad \mathsf{let}\; ( sex, chan )\; \mathsf{=}\; ( \mathsf{rand}( 0, 1 ), \mathsf{choose}( N, D ) ) \;\mathsf{in}\; \\
        &\quad\quad\quad\quad \mathsf{let}\; pheno = \\
          &\quad\quad\quad\quad\quad\quad \mathsf{match}\; sex\; \mathsf{with}\; \{ \\
            &\quad\quad\quad\quad\quad\quad\quad \mathsf{case}\; 0\; \Rightarrow\; \{ \\
              &\quad\quad\quad\quad\quad\quad\quad\quad \mathsf{for}( t\mathsf{@}\mathsf{Theory}\;\{ \mathsf{terms}( e ), \mathsf{equations}( e ), r \} \;\leftarrow\; chan )\{ \\
                &\quad\quad\quad\quad\quad\quad\quad\quad\quad Locate( e \Join t, { chan }, \mathsf{Delta}( chan ) ) \\
              &\quad\quad\quad\quad\quad\quad\quad\quad \} \\
            &\quad\quad\quad\quad\quad\quad\quad \} \\
            &\quad\quad\quad\quad\quad\quad\quad \mathsf{case}\; 1\; \Rightarrow \{ chan\mathsf{!}( e ) \} \\
         &\quad\quad\quad\quad\quad\quad \} \;\mathsf{in}\; \\
          &\quad\quad\quad\quad\quad\quad\quad acc \; \mathsf{|}\; pheno \\
      &\quad\quad\quad \} \\
    &\quad\quad ) \; \mathsf{in}\; \\
   &\quad\quad \mathsf{free}( \mathsf{MeTTaIL} ) \; \Rightarrow \; \{ \\
      &\quad\quad\quad Locate( Pop, N, D ) \\
   &\quad\quad\} 
\end{align*}

Note that this preserves the core action of each theory, but changes the
contextual wrapper around this action. Compare this with biology. Once
chemistry had kindly supplied the Krebs cycle, this core action is conserved
across all life. Likewise, once biology had kindly supplied sexual
reproduction, this core action is conserved across all sexually reproducing
life forms.

\section{A few use cases}

\subsection{Private data and models-as-a-service marketplace}

As mentioned in the requirements analysis, we posit that the next
generation of models will be trained on privately held data. The
biggest shift, in our view, will be towards a marketplace in which
companies with data and companies with model infrastructure will be
able to consume one another's services. $\MC$ is aimed at enabling
this marketplace. Beyond these we see many other use cases and list a
few below.

\subsection{Ocaps for DevOps PaaS}

A particularly compelling application of the $\MC$ architecture lies
in the DevOps Platform-as-a-Service (PaaS) sector, where teams seek to
configure and monitor projects with minimal overhead. Developers often
rely on centralized providers to share infrastructure and
capabilities, which can result in vulnerability to deplatforming (for
example, when payment providers or credit card companies refuse
service) or require complicated permissioning frameworks.

By incorporating object-capabilities (ocaps) into a DevOps
environment, $\MC$ could provide a robust mechanism for delegating and
attenuating authorization among team members. Specifically, a
lightweight plugin or extension built on top of $\MC$ could let
organizations share their DevOps resources (such as CI/CD pipelines,
artifact repositories, and monitoring dashboards) via secure
tokens. This would allow fine-grained access control---even down to
the level of revocable, limited-lifespan tokens for specific tasks.

This approach distinguishes itself from existing DevOps PaaS offerings
in that it deploys ocaps as the fundamental security model, rather
than as an add-on or ad-hoc wrapper. While no current competitor
offers a sharing model based on ocaps, the development of such a
plugin would likely require its own dedicated team to ensure seamless
integration with existing DevOps tools.

An interesting extension of this model would be to empower autonomous
AI agents with delegated ocaps for various DevOps operations. For
example, an AI-driven build optimizer could be granted a strictly
bounded token that allows it to adjust continuous integration
parameters without risking broader system access. Such autonomous
agents could also facilitate zero-downtime deployments and real-time
resource scaling, all while ensuring that their authority remains
precisely delimited by the ocaps architecture.

\subsection{Sensors}

A second use case envisions a network of industrial sensors within a
smart factory, all powered by $\MC$'s concurrency and transactional
semantics. Many factories rely on streams of data from robotic arms,
assembly lines, and inventory management systems, feeding downstream
analytics to optimize production. A decentralized “marketplace” for
sensor data could allow different agents---both human and AI---to
connect, filter, and aggregate these streams.

Using $\MC$, sensors and agents become active participants in an
economy of data consumption. The architecture’s flexible query
language can filter live streams and route relevant information to
different processes for anomaly detection or predictive
maintenance. The emphasis on concurrency and fine-grained access
tokens could enable secure, selective data sharing among industrial
partners or even across entire supply chains.

In this environment, autonomous AI agents could automatically
negotiate data access or deploy new analytics modules where they
detect certain operational inefficiencies. These agents might, for
instance, purchase sensor data streams to train local machine-learning
models or dynamically grant short-term access credentials to other
agents for collaborative filtering. Crucially, the ocaps mechanism
ensures they can only access precisely what they are authorized to,
preventing them from overstepping their operational bounds.

\subsection{Sandstorm}

Sandstorm (sandstorm.org) has long provided a platform for sandboxed
apps that users can self-host, ensuring data ownership. By deploying
$\MC$ as an underlying engine for sandstorm deployments, resource
owners can store, share, and collaboratively edit data in a
decentralized fashion. Attenuation and delegation of capabilities
would be governed by tokens: the owner (or a controlling agent) issues
tokens to participants, granting privileged or time-limited access to
certain documents, files, or worlds. This model aligns with
sandstorm.org’s mission of self-hosting and privacy protection,
extending it with the added benefits of a censorship-resistant,
Byzantine fault tolerant infrastructure.

Because the integration would require modifications or plugin
development within the sandstorm stack itself, it might entail a
dedicated development effort. The result, however, would unify the
user-friendly collaboration tools sandstorm.org is known for with the
secure, decentralized capabilities of $\MC$.

AI agents could automate content moderation tasks, track and summarize
collaborative work, or even autonomously curate shared documents for
improved knowledge management. By granting these agents time-limited
tokens to view and potentially modify files, users could safely
outsource tasks such as grammar correction, code review, or content
organization to intelligent assistants without risking full
administrative access over an entire collection.

\subsection{File hosting}

Finally, an obvious and powerful use case for $\MC$ is straightforward
file hosting and sharing, akin to Dropbox or Google Drive. Current
commercial solutions often rely on crash fault tolerant architectures
that scale well but are prone to deplatforming or security breaches at
the central provider. There are also open-source alternatives, such as
Tahoe-LAFS, that offer strong decentralization and encryption. $\MC$
naturally aligns with the design philosophy behind
censorship-resistant file systems, providing concurrency,
transactionality, and token-based access.

Leveraging $\FD$'s existing filesystem bridge, a user could simply
drag and drop documents into a folder that is mapped onto $\MC$. This
approach already offers encryption at rest, with $\FD$'s environment
allowing multi-party access via tokenization. The result is a file
hosting platform in which censorship is far more difficult, and
availability is improved through blockchain-level fault-tolerance. A
key question for future work is how to best integrate advanced
encryption and chunk-level redundancy strategies (similar to
Tahoe-LAFS) into $\FD$'s architecture to further enhance privacy and
reliability.

In this file hosting scenario, autonomous AI agents could assist with
document indexing, advanced search capabilities, and automated
backups. Using token-limited capabilities, agents might be granted
read-only access to large sets of stored files, enabling them to
provide semantic search or categorization without the risk of
unauthorized edits or deletions. This delegation approach would
preserve user control while unlocking AI-driven efficiencies in
managing and organizing decentralized data.

\section{Conclusion}

If we focus just on the language design for $\mathsf{MeTTa}$ we see
that our proposed architecture addresses the defects of the current
$\mathsf{MeTTa}$ implementation:

\begin{itemize}
    \item GSLTs can specify the semantics of any discrete theory of computation
    \item From a GSLT, we can generate an efficient interpreter for that theory
    \item From a GSLT, we can generate a sound type system customized to that theory
    \item GSLTs allow defining functions inline.
    \item GSLTs let the programmer specify exactly when subexpressions can reduce.
    \item Rather than have a single shared fact database, the facts become part of a query, and the intepreter can produce new fact databases in in response to queries.
    \item Theories can be combined in multiple ways, including concurrent execution.
    \item State updates are transactional.
    \item The automatically generated type systems are provably sound.
    \item When two theories are combined, the system can automatically generate a new type system encompassing both and their interactions.
\end{itemize}

But the goal of $\MC$ is to decentralize agentic computing. That means
much more than simply improving the language. Specifically, it means
situating a high-speed $\mathsf{MeTTa}$ kernel into a tokenization
container, much like the EVM situates a more or less standard vm into
a tokenization container; and then wrapping that in a unit of
fault-tolerance, i.e. a shard governed by a consensus mechanism.

Unlike the EVM, the $\MC$ architecture goes a long way to
demonstrating that the vast majority of the wrapper around smart
contract execution need not by-hand or bespoke software. The
tokenization mechanism, for example, may be derived directly from the
operational semantics of a given language. Likewise, once one
undertands what corresponds to a state change, even in the presence of
non-determinism, the consensus mechanism may be entirely
componentized. Regardless of how consensus is achieved what it must
guarantee is that each node agrees each state change. But a state
change is captured directly in the rewrite rules of an operational
semantics. In short, any language equipped with an executable
operational semantics may be turned into a smart contracting language
in a completely automated fashion.

$\mathsf{MeTTaIL}$ provides a way to write down an executable
operational semantics for all finitely presentable GSLTs. In other
words, $\MC$ reduces these mathematical facts to practice, ushering a
new wave of decentralization in which nodes may participate in
multiple different networks. This promiscuity dramatically increases
the opportunities for network interoperability, and allows new
features to be brought from one network to another.

More generally, the ability to autogenerate the bulk of the
technological apparatus that goes into decentralization lowers the bar
for the development and deployment of decentralized solutions. Instead
of spending years developing a smart contracting system by hand, or
months forking and tweaking an existing system, the $\MC$ provides a
collection of algorithms that can reduce this part of the effort of
rolling out a new network to minutes. From an existing operational
semantics spec, press a button \emph{et voila} an efficient
\emph{tokenized} interpreter of the semantics is produced as
executable code. Press another button \emph{et voila} a
spatial-behavioral type system and an efficient type-checker for it is
rendered to code, thus delivering safety, liveness, and security
guarantees that the current generation of smart contracts could have
greatly benefitted from. Finally, merely choose the form of consensus
needed for the contemplated deployment use case from a library of
existing components, and \emph{hey presto!} your network is ready for
deployment.

By commoditizing these aspects of the technical challenges of
providing decentralized solutions decentralization itself is further
decentralized. With $\MC$ anyone can decentralize their
offering. Reasons not to are pushed further into sociological or
political spheres, not technical ones. After all, nature is both
master and mistress of decentralization. Birds do it, bees do it, why
can't we do it?

As we have emphasized, however, for the full potential of
decentralization to be realized it must be integrated into the
existing technological ecosystem so that users can use the technology
as part of their day-to-day workflows, both on their local devices, and in
the cloud. Currently, this does require bespoke development. For
example, the existing workflows involving file systems, as well as
ones involving social media are completely oblivious to
tokenization. As a result, integration requires more ad hoc and
bespoke solutions. Over time, however, who knows, perhaps even these
integrations can be automated.


\begin{thebibliography}{99}

\bibitem{Barendregt1991}
Barendregt, Henk (1991). 
``Introduction to generalized type systems.''  
\emph{Journal of Functional Programming}, 1(2): 125--154.

\bibitem{Caires2007}
Caires, Luis (2007). 
``Logical Semantics of Types for Concurrency.'' 
In \emph{Algebra and Coalgebra in Computer Science}, 
Second International Conference, CALCO 2007, Bergen, Norway, August 20--24, 2007, Proceedings.
\url{http://ctp.di.fct.unl.pt/~lcaires/papers/CALCO-Caires-1.0.pdf}

\bibitem{CardelliGordon2000}
Cardelli, Luca, and Andrew D. Gordon (2000). ``Mobile ambients.'' \emph{Theoretical Computer Science}, 240: 177--213.

\bibitem{GoertzelMeredith2024}
Goertzel, Ben, and L. Gregory Meredith. Private communication.

\bibitem{HypercubePaper}
[Placeholder for the Hypercube functor paper reference; “cite the hypercube functor paper”].

\bibitem{kframework}
K Framework.
\url{http://kframework.org}.

\bibitem{Lawvere1963}
Lawvere, William (1963). 
``Functorial Semantics of Algebraic Theories'' (PhD Thesis). 
\url{http://www.tac.mta.ca/tac/reprints/articles/5/tr5abs.html}.

\bibitem{MeredithRadestock2005}
Meredith, L.~Gregory, and Matthias Radestock (2005). 
``A Reflective Higher-order Calculus.'' 
\emph{Electronic Notes in Theoretical Computer Science} 141(5), 49--67, 22 December 2005. 
\url{https://doi.org/10.1016/j.entcs.2005.05.016}

\bibitem{Stiegler2004}
Stiegler, Marc (2004). \emph{A PictureBook of Secure Cooperation}. \url{http://www.erights.org/talks/efun/SecurityPictureBook.pdf}.

\bibitem{Tate2010}
Tate, Ross, Michael Stepp, and Sorin Lerner (2010). 
``Generating Compiler Optimization from Proofs.'' 
In \emph{POPL '10: Proceedings of the 37th annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}, 
pp.\ 389--402, Madrid, Spain. 
ACM, New York, NY, USA. 
\url{http://www.cs.cornell.edu/~ross/publications/proofgen/}.

\bibitem{Trimble2018}
Trimble, Todd (2018). 
``Multisorted Lawvere Theories,'' on the nLab. 
\url{https://ncatlab.org/toddtrimble/published/multisorted+Lawvere+theories}.

\bibitem{WilliamsStay2021}
Williams, Christian, and Michael Stay (2021). ``Native Type Theory.'' \emph{Proceedings of the Fourth Annual Conference on Applied Category Theory}. EPTCS 372 (2022), 116--132.

\end{thebibliography}

\section{Appendix A: A Summary of $\mathsf{MeTTaIL}$}
\label{Appendix A}

\setlength{\parindent}{0mm}
\setlength{\parskip}{1mm}

\newcommand{\emptyP}{\mbox{$\epsilon$}}
\newcommand{\terminal}[1]{\mbox{{\texttt {#1}}}}
\newcommand{\nonterminal}[1]{\mbox{$\langle \mbox{{\sl #1 }} \! \rangle$}}
\newcommand{\arrow}{\mbox{::=}}
\newcommand{\delimit}{\mbox{$|$}}
\newcommand{\reserved}[1]{\mbox{{\texttt {#1}}}}
\newcommand{\literal}[1]{\mbox{{\texttt {#1}}}}
\newcommand{\symb}[1]{\mbox{{\texttt {#1}}}}

This appendix was automatically generated by the {\em BNF-Converter}.
It was generated together with the lexer, the parser, and the
abstract syntax module, which guarantees that the document
matches with the implementation of the language
(provided no hand-hacking has taken place).

\section{The lexical structure of $\mathsf{MeTTaIL}$}

\subsection{Identifiers}
Identifiers \nonterminal{Ident} are unquoted strings beginning with a letter,
followed by any combination of letters, digits, and the characters {\tt \_ '},
reserved words excluded.
\subsection{Literals}
Character literals \nonterminal{Char}\ have the form
\terminal{'}$c$\terminal{'}, where $c$ is any single character.

Integer literals \nonterminal{Int}\ are nonempty sequences of digits.

String literals \nonterminal{String}\ have the form
\terminal{"}$x$\terminal{"}, where $x$ is any sequence of any characters
except \terminal{"}\ unless preceded by \verb6\6.

\subsection{Reserved words and symbols}
The set of reserved words is the set of terminals appearing in the grammar. Those reserved words that consist of non-letter characters are called symbols, and they are treated in a different way from those that are similar to identifiers. The lexer follows rules familiar from languages like Haskell, C, and Java, including longest match and spacing conventions.

The reserved words used in $\mathsf{MeTTaIL}$ are the following: \\

\begin{tabular}{lll}
{\reserved{Bind}} &{\reserved{Empty}} &{\reserved{Equations}} \\
{\reserved{Exports}} &{\reserved{Module}} &{\reserved{NilSpace}} \\
{\reserved{Replacements}} &{\reserved{Rewrites}} &{\reserved{Space}} \\
{\reserved{Subst}} &{\reserved{Terms}} &{\reserved{Theory}} \\
{\reserved{as}} &{\reserved{char}} &{\reserved{coercions}} \\
{\reserved{comment}} &{\reserved{digit}} &{\reserved{entrypoints}} \\
{\reserved{eps}} &{\reserved{for}} &{\reserved{free}} \\
{\reserved{from}} &{\reserved{if}} &{\reserved{import}} \\
{\reserved{in}} &{\reserved{inf}} &{\reserved{internal}} \\
{\reserved{layout}} &{\reserved{let}} &{\reserved{letter}} \\
{\reserved{lower}} &{\reserved{nonempty}} &{\reserved{position}} \\
{\reserved{rules}} &{\reserved{self}} &{\reserved{separator}} \\
{\reserved{space}} &{\reserved{stop}} &{\reserved{sup}} \\
{\reserved{tail}} &{\reserved{terminator}} &{\reserved{then}} \\
{\reserved{theory}} &{\reserved{token}} &{\reserved{toplevel}} \\
{\reserved{upper}} & & \\
\end{tabular}\\

The symbols used in $\mathsf{MeTTaIL}$ are the following: \\

\begin{tabular}{lll}
{\symb{\{}} &{\symb{\}}} &{\symb{(}} \\
{\symb{)}} &{\symb{{$=$}{$>$}}} &{\symb{{$=$}}} \\
{\symb{::}} &{\symb{/$\backslash$}} &{\symb{$\backslash$/}} \\
{\symb{$\backslash$}} &{\symb{{$<$}{$|$}}} &{\symb{**}} \\
{\symb{{$|$}}} &{\symb{,}} &{\symb{:}} \\
{\symb{.}} &{\symb{\_}} &{\symb{...}} \\
{\symb{@}} &{\symb{;}} &{\symb{!?}} \\
{\symb{{$<$}{$-$}}} &{\symb{\&}} &{\symb{?!}} \\
{\symb{{$<$}{$=$}}} &{\symb{{$<$}{$<$}{$-$}}} &{\symb{{$<$}{$-$}{$>$}}} \\
{\symb{{$<$}{$=$}{$>$}}} &{\symb{{$<$}{$<$}{$-$}{$>$}{$>$}}} &{\symb{!}} \\
{\symb{!!}} &{\symb{!\$}} &{\symb{::{$=$}}} \\
{\symb{[}} &{\symb{]}} &{\symb{{$-$}}} \\
{\symb{*}} &{\symb{{$+$}}} &{\symb{?}} \\
{\symb{\#}} &{\symb{{$=$}{$=$}}} &{\symb{\~{}{$>$}}} \\
\end{tabular}\\

\subsection{Comments}
Single-line comments begin with {\symb{{$-$}{$-$}}}. \\Multiple-line comments are  enclosed with {\symb{\{{$-$}}} and {\symb{{$-$}\}}}.

\section{The syntactic structure of $\mathsf{MeTTaIL}$}


Non-terminals are enclosed between $\langle$ and $\rangle$.
The symbols  {\arrow}  (production),  {\delimit}  (union)
and {\emptyP} (empty rule) belong to the BNF notation.
All other symbols are terminals.\\

\begin{tabular}{lll}
{\nonterminal{Module}} & {\arrow}  &{\nonterminal{ListImport}} {\terminal{Module}} {\nonterminal{Name}} {\terminal{\{}} {\nonterminal{ListProg}} {\terminal{\}}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Import}} & {\arrow}  &{\terminal{import}} {\nonterminal{String}} {\terminal{as}} {\nonterminal{Ident}}  \\
 & {\delimit}  &{\terminal{import}} {\nonterminal{Ident}} {\terminal{from}} {\nonterminal{String}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListImport}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Import}} {\nonterminal{ListImport}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Prog}} & {\arrow}  &{\nonterminal{SpaceDecl}}  \\
 & {\delimit}  &{\nonterminal{TheoryDecl}}  \\
 & {\delimit}  &{\terminal{space}} {\nonterminal{SpaceInst}}  \\
 & {\delimit}  &{\terminal{theory}} {\nonterminal{TheoryInst}}  \\
 & {\delimit}  &{\nonterminal{FactComprehension}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListProg}} & {\arrow}  &{\nonterminal{Prog}}  \\
 & {\delimit}  &{\nonterminal{Prog}} {\nonterminal{ListProg}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst}} & {\arrow}  &{\nonterminal{SpaceInst1}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst}} {\terminal{{$|$}}} {\nonterminal{SpaceInst1}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst1}} & {\arrow}  &{\nonterminal{SpaceInst2}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst1}} {\terminal{**}} {\nonterminal{SpaceInst2}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst2}} & {\arrow}  &{\nonterminal{SpaceInst3}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst2}} {\terminal{{$<$}{$|$}}} {\nonterminal{SpaceInst3}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst3}} & {\arrow}  &{\nonterminal{SpaceInst4}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst3}} {\terminal{$\backslash$}} {\nonterminal{SpaceInst4}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst4}} & {\arrow}  &{\nonterminal{SpaceInst5}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst4}} {\terminal{$\backslash$/}} {\nonterminal{SpaceInst5}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst5}} & {\arrow}  &{\nonterminal{SpaceInst6}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst5}} {\terminal{/$\backslash$}} {\nonterminal{SpaceInst6}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst6}} & {\arrow}  &{\nonterminal{SpaceInst7}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst6}} {\terminal{::}} {\nonterminal{SpaceInst7}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceInst7}} & {\arrow}  &{\terminal{\{}} {\nonterminal{SpaceInst}} {\terminal{\}}}  \\
 & {\delimit}  &{\terminal{NilSpace}}  \\
 & {\delimit}  &{\nonterminal{DottedPath}} {\terminal{(}} {\nonterminal{ListSpaceInst}} {\terminal{)}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst7}} {\terminal{{$=$}{$>$}}} {\terminal{\{}} {\nonterminal{ListProg}} {\terminal{\}}}  \\
 & {\delimit}  &{\nonterminal{Ident}}  \\
 & {\delimit}  &{\terminal{let}} {\nonterminal{Ident}} {\terminal{{$=$}}} {\nonterminal{SpaceInst7}} {\terminal{in}} {\terminal{(}} {\nonterminal{SpaceInst7}} {\terminal{)}}  \\
 & {\delimit}  &{\terminal{tail}} {\terminal{(}} {\nonterminal{SpaceInst7}} {\terminal{)}}  \\
 & {\delimit}  &{\terminal{sup}} {\terminal{(}} {\nonterminal{SpaceInst7}} {\terminal{)}}  \\
 & {\delimit}  &{\terminal{inf}} {\terminal{(}} {\nonterminal{SpaceInst7}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListSpaceInst}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{SpaceInst}}  \\
 & {\delimit}  &{\nonterminal{SpaceInst}} {\terminal{,}} {\nonterminal{ListSpaceInst}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryInst}} & {\arrow}  &{\nonterminal{TheoryInst1}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryInst1}} & {\arrow}  &{\nonterminal{TheoryInst2}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst1}} {\terminal{{$<$}{$|$}}} {\nonterminal{TheoryInst2}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryInst2}} & {\arrow}  &{\nonterminal{TheoryInst3}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst2}} {\terminal{$\backslash$}} {\nonterminal{TheoryInst3}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryInst3}} & {\arrow}  &{\nonterminal{TheoryInst4}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst3}} {\terminal{$\backslash$/}} {\nonterminal{TheoryInst4}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryInst4}} & {\arrow}  &{\nonterminal{TheoryInst5}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst4}} {\terminal{/$\backslash$}} {\nonterminal{TheoryInst5}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryInst5}} & {\arrow}  &{\nonterminal{TheoryInst6}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst5}} {\terminal{Exports}} {\terminal{\{}} {\nonterminal{ListExport}} {\terminal{\}}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst5}} {\terminal{Replacements}} {\terminal{\{}} {\nonterminal{ListReplacement}} {\terminal{\}}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst5}} {\terminal{Terms}} {\terminal{\{}} {\nonterminal{Grammar}} {\terminal{\}}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst5}} {\terminal{Equations}} {\terminal{\{}} {\nonterminal{ListEquation}} {\terminal{\}}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst5}} {\terminal{Rewrites}} {\terminal{\{}} {\nonterminal{ListRewriteDecl}} {\terminal{\}}}  \\
 & {\delimit}  &{\nonterminal{DottedPath}} {\terminal{(}} {\nonterminal{ListTheoryInst}} {\terminal{)}}  \\
 & {\delimit}  &{\nonterminal{Ident}}  \\
 & {\delimit}  &{\terminal{let}} {\nonterminal{Ident}} {\terminal{{$=$}}} {\nonterminal{TheoryInst5}} {\terminal{in}} {\terminal{(}} {\nonterminal{TheoryInst5}} {\terminal{)}}  \\
 & {\delimit}  &{\terminal{free}} {\terminal{(}} {\nonterminal{DottedPath}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryInst6}} & {\arrow}  &{\terminal{\{}} {\nonterminal{TheoryInst}} {\terminal{\}}}  \\
 & {\delimit}  &{\terminal{Empty}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListTheoryInst}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{TheoryInst}}  \\
 & {\delimit}  &{\nonterminal{TheoryInst}} {\terminal{,}} {\nonterminal{ListTheoryInst}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SpaceDecl}} & {\arrow}  &{\terminal{Space}} {\nonterminal{Name}} {\terminal{(}} {\nonterminal{ListVariableDecl}} {\terminal{)}} {\terminal{\{}} {\nonterminal{ListFact}} {\nonterminal{ListFactComprehension}} {\terminal{\}}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{TheoryDecl}} & {\arrow}  &{\terminal{Theory}} {\nonterminal{Name}} {\terminal{(}} {\nonterminal{ListVariableDecl}} {\terminal{)}} {\terminal{\{}} {\nonterminal{TheoryInst}} {\terminal{\}}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{VariableDecl}} & {\arrow}  &{\nonterminal{Ident}} {\terminal{:}} {\nonterminal{DottedPath}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListVariableDecl}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{VariableDecl}}  \\
 & {\delimit}  &{\nonterminal{VariableDecl}} {\terminal{,}} {\nonterminal{ListVariableDecl}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{DottedPath}} & {\arrow}  &{\nonterminal{Ident}}  \\
 & {\delimit}  &{\nonterminal{Ident}} {\terminal{.}} {\nonterminal{DottedPath}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Name}} & {\arrow}  &{\terminal{\_}}  \\
 & {\delimit}  &{\nonterminal{Ident}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListName}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Name}}  \\
 & {\delimit}  &{\nonterminal{Name}} {\terminal{,}} {\nonterminal{ListName}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{NameRemainder}} & {\arrow}  &{\terminal{...}} {\terminal{@}} {\nonterminal{Ident}}  \\
 & {\delimit}  &{\emptyP} \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Fact}} & {\arrow}  &{\nonterminal{Item}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListFact}} & {\arrow}  &{\nonterminal{Fact}}  \\
 & {\delimit}  &{\nonterminal{Fact}} {\terminal{;}} {\nonterminal{ListFact}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{FactComprehension}} & {\arrow}  &{\terminal{for}} {\terminal{(}} {\nonterminal{ListReceipt}} {\terminal{)}} {\nonterminal{SpaceInst7}}  \\
 & {\delimit}  &{\nonterminal{Name}} {\nonterminal{Send}} {\terminal{(}} {\nonterminal{ListFact}} {\terminal{)}}  \\
 & {\delimit}  &{\nonterminal{Name}} {\nonterminal{Send}} {\terminal{(}} {\terminal{)}}  \\
 & {\delimit}  &{\nonterminal{Name}} {\terminal{!?}} {\terminal{(}} {\nonterminal{ListFact}} {\terminal{)}} {\nonterminal{SynchSendCont}}  \\
 & {\delimit}  &{\nonterminal{Name}} {\terminal{!?}} {\terminal{(}} {\terminal{)}} {\nonterminal{SynchSendCont}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListFactComprehension}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{FactComprehension}}  \\
 & {\delimit}  &{\nonterminal{FactComprehension}} {\terminal{,}} {\nonterminal{ListFactComprehension}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Receipt}} & {\arrow}  &{\nonterminal{ReceiptLinearImpl}}  \\
 & {\delimit}  &{\nonterminal{ReceiptRepeatedImpl}}  \\
 & {\delimit}  &{\nonterminal{ReceiptPeekImpl}}  \\
 & {\delimit}  &{\nonterminal{ReceiptSymmLinearImpl}}  \\
 & {\delimit}  &{\nonterminal{ReceiptSymmRepeatedImpl}}  \\
 & {\delimit}  &{\nonterminal{ReceiptSymmPeekImpl}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListReceipt}} & {\arrow}  &{\nonterminal{Receipt}}  \\
 & {\delimit}  &{\nonterminal{Receipt}} {\terminal{;}} {\nonterminal{ListReceipt}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ReceiptLinearImpl}} & {\arrow}  &{\nonterminal{ListLinearBind}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{LinearBind}} & {\arrow}  &{\nonterminal{ListName}} {\nonterminal{NameRemainder}} {\terminal{{$<$}{$-$}}} {\nonterminal{NameSource}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListLinearBind}} & {\arrow}  &{\nonterminal{LinearBind}}  \\
 & {\delimit}  &{\nonterminal{LinearBind}} {\terminal{\&}} {\nonterminal{ListLinearBind}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{NameSource}} & {\arrow}  &{\nonterminal{Name}}  \\
 & {\delimit}  &{\terminal{self}}  \\
 & {\delimit}  &{\nonterminal{Name}} {\terminal{?!}}  \\
 & {\delimit}  &{\nonterminal{Name}} {\terminal{!?}} {\terminal{(}} {\nonterminal{ListFact}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ReceiptRepeatedImpl}} & {\arrow}  &{\nonterminal{ListRepeatedBind}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{RepeatedBind}} & {\arrow}  &{\nonterminal{ListName}} {\nonterminal{NameRemainder}} {\terminal{{$<$}{$=$}}} {\nonterminal{Name}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListRepeatedBind}} & {\arrow}  &{\nonterminal{RepeatedBind}}  \\
 & {\delimit}  &{\nonterminal{RepeatedBind}} {\terminal{\&}} {\nonterminal{ListRepeatedBind}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ReceiptPeekImpl}} & {\arrow}  &{\nonterminal{ListPeekBind}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{PeekBind}} & {\arrow}  &{\nonterminal{ListName}} {\nonterminal{NameRemainder}} {\terminal{{$<$}{$<$}{$-$}}} {\nonterminal{Name}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListPeekBind}} & {\arrow}  &{\nonterminal{PeekBind}}  \\
 & {\delimit}  &{\nonterminal{PeekBind}} {\terminal{\&}} {\nonterminal{ListPeekBind}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ReceiptSymmLinearImpl}} & {\arrow}  &{\nonterminal{ListLinearBindSymm}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{LinearBindSymm}} & {\arrow}  &{\nonterminal{ListName}} {\nonterminal{NameRemainder}} {\terminal{{$<$}{$-$}{$>$}}} {\nonterminal{NameSource}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListLinearBindSymm}} & {\arrow}  &{\nonterminal{LinearBindSymm}}  \\
 & {\delimit}  &{\nonterminal{LinearBindSymm}} {\terminal{\&}} {\nonterminal{ListLinearBindSymm}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ReceiptSymmRepeatedImpl}} & {\arrow}  &{\nonterminal{ListRepeatedBindSymm}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{RepeatedBindSymm}} & {\arrow}  &{\nonterminal{ListName}} {\nonterminal{NameRemainder}} {\terminal{{$<$}{$=$}{$>$}}} {\nonterminal{Name}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListRepeatedBindSymm}} & {\arrow}  &{\nonterminal{RepeatedBindSymm}}  \\
 & {\delimit}  &{\nonterminal{RepeatedBindSymm}} {\terminal{\&}} {\nonterminal{ListRepeatedBindSymm}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ReceiptSymmPeekImpl}} & {\arrow}  &{\nonterminal{ListPeekBindSymm}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{PeekBindSymm}} & {\arrow}  &{\nonterminal{ListName}} {\nonterminal{NameRemainder}} {\terminal{{$<$}{$<$}{$-$}{$>$}{$>$}}} {\nonterminal{Name}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListPeekBindSymm}} & {\arrow}  &{\nonterminal{PeekBindSymm}}  \\
 & {\delimit}  &{\nonterminal{PeekBindSymm}} {\terminal{\&}} {\nonterminal{ListPeekBindSymm}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{SynchSendCont}} & {\arrow}  &{\terminal{.}}  \\
 & {\delimit}  &{\terminal{;}} {\nonterminal{SpaceInst}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Send}} & {\arrow}  &{\terminal{!}}  \\
 & {\delimit}  &{\terminal{!!}}  \\
 & {\delimit}  &{\terminal{!\$}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Export}} & {\arrow}  &{\nonterminal{Cat}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListExport}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Export}} {\terminal{;}} {\nonterminal{ListExport}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Replacement}} & {\arrow}  &{\nonterminal{IntList}} {\nonterminal{Label}} {\terminal{.}} {\nonterminal{Cat}} {\terminal{{$=$}{$>$}}} {\nonterminal{Def}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListReplacement}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Replacement}} {\terminal{;}} {\nonterminal{ListReplacement}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Grammar}} & {\arrow}  &{\nonterminal{ListDef}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListDef}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Def}} {\terminal{;}} {\nonterminal{ListDef}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListItem}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Item}} {\nonterminal{ListItem}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Def}} & {\arrow}  &{\nonterminal{Label}} {\terminal{.}} {\nonterminal{Cat}} {\terminal{::{$=$}}} {\nonterminal{ListItem}}  \\
 & {\delimit}  &{\terminal{comment}} {\nonterminal{String}}  \\
 & {\delimit}  &{\terminal{comment}} {\nonterminal{String}} {\nonterminal{String}}  \\
 & {\delimit}  &{\terminal{internal}} {\nonterminal{Label}} {\terminal{.}} {\nonterminal{Cat}} {\terminal{::{$=$}}} {\nonterminal{ListItem}}  \\
 & {\delimit}  &{\terminal{token}} {\nonterminal{Ident}} {\nonterminal{Reg}}  \\
 & {\delimit}  &{\terminal{position}} {\terminal{token}} {\nonterminal{Ident}} {\nonterminal{Reg}}  \\
 & {\delimit}  &{\terminal{entrypoints}} {\nonterminal{ListIdent}}  \\
 & {\delimit}  &{\terminal{separator}} {\nonterminal{MinimumSize}} {\nonterminal{Cat}} {\nonterminal{String}}  \\
 & {\delimit}  &{\terminal{terminator}} {\nonterminal{MinimumSize}} {\nonterminal{Cat}} {\nonterminal{String}}  \\
 & {\delimit}  &{\terminal{coercions}} {\nonterminal{Ident}} {\nonterminal{Integer}}  \\
 & {\delimit}  &{\terminal{rules}} {\nonterminal{Ident}} {\terminal{::{$=$}}} {\nonterminal{ListRHS}}  \\
 & {\delimit}  &{\terminal{layout}} {\nonterminal{ListString}}  \\
 & {\delimit}  &{\terminal{layout}} {\terminal{stop}} {\nonterminal{ListString}}  \\
 & {\delimit}  &{\terminal{layout}} {\terminal{toplevel}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Item}} & {\arrow}  &{\nonterminal{String}}  \\
 & {\delimit}  &{\nonterminal{Cat}}  \\
 & {\delimit}  &{\terminal{(}} {\terminal{Bind}} {\nonterminal{Ident}} {\nonterminal{IntList}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Cat}} & {\arrow}  &{\terminal{[}} {\nonterminal{Cat}} {\terminal{]}}  \\
 & {\delimit}  &{\nonterminal{Ident}}  \\
 & {\delimit}  &{\nonterminal{Ident}} {\terminal{.}} {\nonterminal{Cat}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Label}} & {\arrow}  &{\nonterminal{LabelId}}  \\
 & {\delimit}  &{\nonterminal{LabelId}} {\nonterminal{ListProfItem}}  \\
 & {\delimit}  &{\nonterminal{LabelId}} {\nonterminal{LabelId}} {\nonterminal{ListProfItem}}  \\
 & {\delimit}  &{\nonterminal{LabelId}} {\nonterminal{LabelId}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{LabelId}} & {\arrow}  &{\nonterminal{Ident}}  \\
 & {\delimit}  &{\terminal{\_}}  \\
 & {\delimit}  &{\terminal{[}} {\terminal{]}}  \\
 & {\delimit}  &{\terminal{(}} {\terminal{:}} {\terminal{)}}  \\
 & {\delimit}  &{\terminal{(}} {\terminal{:}} {\terminal{[}} {\terminal{]}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ProfItem}} & {\arrow}  &{\terminal{(}} {\terminal{[}} {\nonterminal{ListIntList}} {\terminal{]}} {\terminal{,}} {\terminal{[}} {\nonterminal{ListInteger}} {\terminal{]}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{IntList}} & {\arrow}  &{\terminal{[}} {\nonterminal{ListInteger}} {\terminal{]}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListInteger}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Integer}}  \\
 & {\delimit}  &{\nonterminal{Integer}} {\terminal{,}} {\nonterminal{ListInteger}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListIntList}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{IntList}}  \\
 & {\delimit}  &{\nonterminal{IntList}} {\terminal{,}} {\nonterminal{ListIntList}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListProfItem}} & {\arrow}  &{\nonterminal{ProfItem}}  \\
 & {\delimit}  &{\nonterminal{ProfItem}} {\nonterminal{ListProfItem}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListString}} & {\arrow}  &{\nonterminal{String}}  \\
 & {\delimit}  &{\nonterminal{String}} {\terminal{,}} {\nonterminal{ListString}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListRHS}} & {\arrow}  &{\nonterminal{RHS}}  \\
 & {\delimit}  &{\nonterminal{RHS}} {\terminal{{$|$}}} {\nonterminal{ListRHS}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{RHS}} & {\arrow}  &{\nonterminal{ListItem}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{MinimumSize}} & {\arrow}  &{\terminal{nonempty}}  \\
 & {\delimit}  &{\emptyP} \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Reg2}} & {\arrow}  &{\nonterminal{Reg2}} {\nonterminal{Reg3}}  \\
 & {\delimit}  &{\nonterminal{Reg3}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Reg1}} & {\arrow}  &{\nonterminal{Reg1}} {\terminal{{$|$}}} {\nonterminal{Reg2}}  \\
 & {\delimit}  &{\nonterminal{Reg2}} {\terminal{{$-$}}} {\nonterminal{Reg2}}  \\
 & {\delimit}  &{\nonterminal{Reg2}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Reg3}} & {\arrow}  &{\nonterminal{Reg3}} {\terminal{*}}  \\
 & {\delimit}  &{\nonterminal{Reg3}} {\terminal{{$+$}}}  \\
 & {\delimit}  &{\nonterminal{Reg3}} {\terminal{?}}  \\
 & {\delimit}  &{\terminal{eps}}  \\
 & {\delimit}  &{\nonterminal{Char}}  \\
 & {\delimit}  &{\terminal{[}} {\nonterminal{String}} {\terminal{]}}  \\
 & {\delimit}  &{\terminal{\{}} {\nonterminal{String}} {\terminal{\}}}  \\
 & {\delimit}  &{\terminal{digit}}  \\
 & {\delimit}  &{\terminal{letter}}  \\
 & {\delimit}  &{\terminal{upper}}  \\
 & {\delimit}  &{\terminal{lower}}  \\
 & {\delimit}  &{\terminal{char}}  \\
 & {\delimit}  &{\terminal{(}} {\nonterminal{Reg}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Reg}} & {\arrow}  &{\nonterminal{Reg1}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListIdent}} & {\arrow}  &{\nonterminal{Ident}}  \\
 & {\delimit}  &{\nonterminal{Ident}} {\terminal{,}} {\nonterminal{ListIdent}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Equation}} & {\arrow}  &{\terminal{if}} {\nonterminal{Ident}} {\terminal{\#}} {\nonterminal{Ident}} {\terminal{then}} {\nonterminal{Equation}}  \\
 & {\delimit}  &{\nonterminal{AST}} {\terminal{{$=$}{$=$}}} {\nonterminal{AST}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListEquation}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{Equation}} {\terminal{;}} {\nonterminal{ListEquation}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{RewriteDecl}} & {\arrow}  &{\nonterminal{Ident}} {\terminal{:}} {\nonterminal{Rewrite}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListRewriteDecl}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{RewriteDecl}}  \\
 & {\delimit}  &{\nonterminal{RewriteDecl}} {\terminal{;}} {\nonterminal{ListRewriteDecl}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Rewrite}} & {\arrow}  &{\nonterminal{AST}} {\terminal{\~{}{$>$}}} {\nonterminal{AST}}  \\
 & {\delimit}  &{\terminal{let}} {\nonterminal{Hypothesis}} {\terminal{in}} {\nonterminal{Rewrite}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{AST}} & {\arrow}  &{\terminal{(}} {\terminal{Subst}} {\nonterminal{AST}} {\nonterminal{AST}} {\nonterminal{Ident}} {\terminal{)}}  \\
 & {\delimit}  &{\nonterminal{Ident}}  \\
 & {\delimit}  &{\terminal{(}} {\nonterminal{Ident}} {\nonterminal{ListAST}} {\terminal{)}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{ListAST}} & {\arrow}  &{\emptyP} \\
 & {\delimit}  &{\nonterminal{AST}} {\nonterminal{ListAST}}  \\
\end{tabular}\\

\begin{tabular}{lll}
{\nonterminal{Hypothesis}} & {\arrow}  &{\nonterminal{Ident}} {\terminal{\~{}{$>$}}} {\nonterminal{Ident}}  \\
\end{tabular}\\

\section{Appendix B: Examples of $\mathsf{MeTTaIL}$}
\label{Appendix B}

\begin{verbatim}
//UnivAlg.module file contents
Module UnivAlg {
  Theory TwoPoint() {
    Empty
      Exports {
        Proc;
      }
      Terms {
        One . Proc ::= "1" ;
        Two . Proc ::= "2" ;
      }
  }

  Theory Monoid() {
    Empty
      Exports {
        Proc;
      }
      Terms {
        One . Proc ::= "1" ;
        Mult . Proc ::= "(" Proc "*" Proc ")" ;
      }
      Equations {
        (Mult (Mult x y) z) == (Mult x (Mult y z));
        (Mult x (One)) == x;
        (Mult (One) x) == x;
      }
  }

  Theory CommutativeMonoid(m: u.Monoid) {
    m
      Replacements {
        [] One . Proc => Zero . Proc ::= "0" ;
        [0, 1] Mult . Proc => Plus . Proc ::= "(" Proc "+" Proc ")" ;
      }
      Equations {
        (Plus x y) == (Plus y x);
      }
  }
  
  Theory Rig(add: CommutativeMonoid, mult: Monoid) {
    {add \/ mult}
      Equations {
        (Mult x (Plus y z)) == (Plus (Mult x y) (Mult x z));
        (Mult (Plus x y) z) == (Plus (Mult x z) (Mult y z));
        (Mult x (Zero)) == (Zero);
        (Mult (Zero) x) == (Zero);
      }
  }

  Theory Group(m: Monoid) {
    m
      Terms {
        Inv . Proc ::= "inv" "(" Proc ")" ;
      }
      Equations {
        (Mult x (Inv x)) == (One);
        (Inv (Inv x)) == x;
        (Inv (Mult x y)) == (Mult (Inv y) (Inv x));
      }
  }

  Theory AbelianGroup(g: Group, c: CommutativeMonoid) {
    g
      Replacements {
        [] One . Proc => Zero . Proc ::= "0" ;
        [0, 1] Mult . Proc => Plus . Proc ::= "(" Proc "+" Proc ")" ;
        [0] Inv . Proc => Neg . Proc ::= "(" "-" Proc ")" ;
      } \/ c
  }

  Theory Ring(r: Rig, add: AbelianGroup) {
    r \/ add
  }
  
  theory TwoPoint() \/ Monoid()
}

//Rholang.module file contents

import Monoid from "UnivAlg.module"

Module Rholang {
  Theory ParMonoid(cm: u.CommutativeMonoid) {
    cm
      Replacements {
        [] Zero.Proc => PZero.Proc ::= "0";
        [0, 1] Plus.Proc => PPar.Proc ::= "(" Proc "|" Proc ")";
      }
      Rewrites {
        RPar1 : let Src ~> Tgt in
                ( PPar Src Q ) ~> ( PPar Tgt Q ) ;
        RPar2 : let Src1 ~> Tgt1 in
                let Src2 ~> Tgt2 in
                ( PPar Src1 Src2 ) ~> ( PPar Tgt1 Tgt2 ) ;
      }
  }

  Theory NewReplCalc(pm: ParMonoid) {
    pm
      Exports {
        Name ;
      }
      Terms {
        PRepl . Proc ::= "!" Proc ;
        PNew  . Proc ::= "new" (Bind Name [1]) "in" Proc ;
      }
      Equations {
        if x # Q then
          ( PPar ( PNew x P ) Q ) == ( PNew x ( PPar P Q ) ) ;
        ( PNew x ( PNew x P ) ) == ( PNew x P ) ;
        ( PNew x ( PNew y P ) ) == ( PNew y ( PNew x P ) ) ;
        ( PRepl P ) == ( PPar P ( PRepl P ) ) ;
      }
      Rewrites {
        RNew  : let Src ~> Tgt in
                (PNew x Src) ~> (PNew x Tgt) ;
      }
  }
 
  Theory QuoteDropCalc(pm: ParMonoid) {
    pm
      Exports {
        Name ;
      }
      Terms {
        PDrop . Proc ::= "*" Name ;
        NQuote . Name ::= "@" Proc ;
      }
      Equations {
        ( NQuote ( PDrop N ) ) == N ;
        ( PDrop ( NQuote P ) ) == P ;
      }
  }

  Theory RhoCalc(qd: QuoteDropCalc) {
    qd
      Terms {
        PSend . Proc ::= Name "!" "(" Proc ")" ;
        PRecv . Proc ::= "for" "(" (Bind Name [2]) "<-" Name ")" "{" Proc "}" ;
      }
      Rewrites {
        RComm : ( PPar ( PRecv y x P) ( PSend x Q ) ) ~> ( Subst P ( NQuote Q ) y ) ;
      }
  }

  Theory Rholang(nr: NewReplCalc, r: RhoCalc) {
    nr \/ r
  }
  
  Theory FreeRholang() {
    let m = Monoid() in (
    let cm = CommutativeMonoid(m) in (
    let pm = ParMonoid(cm) in (
    let qd = QuoteDropCalc(pm) in (
    let nr = NewReplCalc(pm) in (
    let rc = Rhocalc(qd) in (
    let rl = Rholang(nr, rc) in (
    r1
    )))))))
  }

  -- theory free(Monoid)
  -- theory free(Rholang)
  -- theory let x = Empty Exports { Foo; } in (x Exports { Bar; })
  theory Monoid()
}
\end{verbatim}

\section{Appendix C: Inference rules expected to be generated by the conjectured hypercube functor acting on the GSLT for RHO calculus}
\label{Appendix C}

\subsection*{1. Axioms, Structural Types}

\[
\forall \text{ shapes } T \text{ in GSLT, } *^T,\, \Box^T: P.
\]

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\vdash *^T : \Box^T$}
\end{prooftree}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{2. Start}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s^T$}
\UnaryInfC{$\Gamma,\, x:A \vdash x : A$}
\end{prooftree}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{3. Weakening}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : B$}
\AxiomC{$\Gamma \vdash C : s^T$}
\BinaryInfC{$\Gamma,\, x:C \vdash A : B$}
\end{prooftree}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{4. Lambda Theory}

\subsubsection*{(a) Dependent Product}

\[
\Lambda : P \times (P \to P) \to P.
\]

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^T$}
\AxiomC{$\Gamma,\, x:A \vdash B : s_2^T$}
\BinaryInfC{$\Gamma \vdash \Lambda(A,\, \lambda x.B) : s_2^{T \to T'}$}
\end{prooftree}

\subsubsection*{(b) Abstraction}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^T$}
\AxiomC{$\Gamma,\, x:A \vdash B : s_2^T$}
\AxiomC{$\Gamma,\, x:A \vdash C : B$}
\TrinaryInfC{$\Gamma \vdash \lambda x.C : \Lambda(A,\, \lambda x.B)$}
\end{prooftree}

\subsubsection*{(c) Application 1}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^T$}
\AxiomC{$\Gamma,\, x:A \vdash B : s_2^T$}
\AxiomC{$\Gamma \vdash D : A$}
\TrinaryInfC{$\Gamma \vdash (\lambda x.B\, D) : s_2^T$}
\end{prooftree}

\subsubsection*{(d) Application 2}

\begin{prooftree}
\AxiomC{$\Gamma \vdash C : \Lambda(A,\, B)$}
\AxiomC{$\Gamma \vdash D : A$}
\BinaryInfC{$\Gamma \vdash (C\, D) : (B\, D)$}
\end{prooftree}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{5. Structural Types and Extending Term Constructors with Exponential Parameters}

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\vdash 0 : s^P$}
\end{prooftree}

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\vdash 0 : 0$}
\end{prooftree}

Here the two sorts are forced to be equal because $|$ is commutative.
\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s^P$}
\AxiomC{$\Gamma \vdash B : s^P$}
\BinaryInfC{$\Gamma \vdash A \mid B : s^P$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s^P$}
\AxiomC{$\Gamma \vdash B : A$}
\AxiomC{$\Gamma \vdash C : s^P$}
\AxiomC{$\Gamma \vdash D : C$}
\QuaternaryInfC{$\Gamma \vdash B \mid D : A \mid C$}
\end{prooftree}

The name $x$ has to be part of the type because the left-hand side of the comm rule duplicates $x$.  It's not yet clear what forces $x!(B)$ to be of the same sort as $x$.  It might be for the same reason, or it might make sense with either choice; if the latter, then there would be a different hypercube functor per choice.
\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^P$}
\TrinaryInfC{$\Gamma \vdash x!(B) : s_1^P$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^P$}
\AxiomC{$\Gamma \vdash Q : B$}
\QuaternaryInfC{$\Gamma \vdash x!(Q) : x!(B)$}
\end{prooftree}

The functor adds an extra parameter to $\rm{for}$ to track the type of the bound variable.
\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^N$}
\AxiomC{$\Gamma,\, y:B \vdash C : s_3^P$}
\QuaternaryInfC{$\Gamma \vdash \rm{for}(x, B, \lambda y.C) : s_1^P$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^N$}
\AxiomC{$\Gamma,\, y:B \vdash C : s_3^P$}
\AxiomC{$\Gamma,\, y:B \vdash D : C$}
\QuinaryInfC{$\Gamma \vdash \rm{for}(x, B, \lambda y.D) : \rm{for}(x, B, \lambda y.C)$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s^N$}
\UnaryInfC{$\Gamma \vdash *A : s^P$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s^N$}
\AxiomC{$\Gamma \vdash x : A$}
\BinaryInfC{$\Gamma \vdash *x : *A$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s^P$}
\UnaryInfC{$\Gamma \vdash @A : s^N$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s^P$}
\AxiomC{$\Gamma \vdash Q : A$}
\BinaryInfC{$\Gamma \vdash @Q : @A$}
\end{prooftree}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{6. App-like Rules for Rewrites with Substitution in RHS}

The base rewrite rule is
\[
\rm{comm}:\quad x!(C) \mid \rm{for}(x, A, B) \leadsto \bigl(B\; @C\bigr).
\]
The $\rm{for}$ term constructor takes a lambda term $B$ as a parameter that's evaluated on the RHS, so we get the following rules:

\subsubsection*{(a) Empty Context}

\[
\Diamond : P \to P.
\]

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : B$}
\UnaryInfC{$\Gamma \vdash A : \Diamond B$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash C : \rm{for}(x, A, B)$}
\AxiomC{$\Gamma \vdash D : *A$}
\BinaryInfC{$\Gamma \vdash C \mid x!(D) : \Diamond (B\; @D)$}
\end{prooftree}

\subsubsection*{(b) LHS Context}

\begin{prooftree}
\AxiomC{$\Gamma \vdash C : \rm{for}(x, A, B)$}
\AxiomC{$\Gamma \vdash D : *A$}
\BinaryInfC{$\Gamma \vdash C : \langle [\rm{for}(x, A, B)] \mid x!(D) \rangle (B\; @D)$}
\end{prooftree}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{7. Modalities for Non-Exponential Positions in Base Rules}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^P$}
\AxiomC{$\Gamma \vdash C : s_2^{P\to P}$}
\AxiomC{$\Gamma \vdash K : C$}
\TrinaryInfC{$\Gamma \vdash \langle K \rangle A : s_1^P$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^P$}
\AxiomC{$\Gamma \vdash C : s_2^{P\to P}$}
\AxiomC{$\Gamma \vdash K : C$}
\AxiomC{$\Gamma \vdash B : \langle K \rangle A$}
\QuaternaryInfC{$\Gamma \vdash (K\, B) : \Diamond A$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^P$}
\AxiomC{$\Gamma \vdash Q : B$}
\AxiomC{$\Gamma,\, y:@B \vdash C : s_3^P$}
\AxiomC{$\Gamma,\, y:@B \vdash D : C$}
\SenaryInfC{$\Gamma \vdash x!(Q) \mid \rm{for}(x, @B, \lambda y.D) : \Diamond \bigl((\lambda y.C)\, @Q\bigr)$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^P$}
\AxiomC{$\Gamma \vdash Q : B$}
\AxiomC{$\Gamma,\, y:@B \vdash C : s_3^P$}
\AxiomC{$\Gamma,\, y:@B \vdash D : C$}
\SenaryInfC{$\Gamma \vdash x!(Q) : \langle [\,x!(B)\,] \mid \rm{for}(x, @B, \lambda y.D) \rangle \bigl((\lambda y.C)\, @Q\bigr)$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^P$}
\AxiomC{$\Gamma \vdash Q : B$}
\AxiomC{$\Gamma,\, y:@B \vdash C : s_3^P$}
\AxiomC{$\Gamma,\, y:@B \vdash D : C$}
\SenaryInfC{$\Gamma \vdash \rm{for}(x, @B, \lambda y.D) : \langle x!(Q) \mid [\,\rm{for}(x, @B, \lambda y.C)\,] \rangle \bigl((\lambda y.C)\, @Q\bigr)$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^N$}
\AxiomC{$\Gamma \vdash x : A$}
\AxiomC{$\Gamma \vdash B : s_2^P$}
\AxiomC{$\Gamma \vdash Q : B$}
\AxiomC{$\Gamma,\, y:@B \vdash C : s_3^P$}
\AxiomC{$\Gamma,\, y:@B \vdash D : C$}
\SenaryInfC{$\Gamma \vdash \lambda y.D : \langle x!(Q) \mid \rm{for}(x, @B, [\,\Lambda(@B,\lambda y.C)\,]) \rangle \bigl((\lambda y.C)\, @Q\bigr)$}
\end{prooftree}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{8. Conv-like Rules}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s$}
\AxiomC{$\Gamma \vdash B : A$}
\AxiomC{$\Gamma \vdash \rho : B \leadsto B'$}
\AxiomC{$\Gamma \vdash B' : A'$}
\QuaternaryInfC{$\Gamma \vdash B : \Diamond A'$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^P$}
\AxiomC{$\Gamma \vdash B : \Diamond A$}
\AxiomC{$\Gamma \vdash C : \langle K[A] \rangle D$}
\TrinaryInfC{$\Gamma \vdash (K\, B) : \Diamond\Diamond D$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : B$}
\UnaryInfC{$\Gamma \vdash (K\, A) : (K\, B)$}
\end{prooftree}

\[
\Diamond^* : P \to P.
\]

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : B$}
\UnaryInfC{$\Gamma \vdash A : \Diamond^* B$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : \Diamond\Diamond^* B$}
\UnaryInfC{$\Gamma \vdash A : \Diamond^* B$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : \Diamond^*\Diamond B$}
\UnaryInfC{$\Gamma \vdash A : \Diamond^* B$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : \Diamond^* \Diamond^* B$}
\UnaryInfC{$\Gamma \vdash A : \Diamond^* B$}
\end{prooftree}

\begin{prooftree}
\AxiomC{$\Gamma \vdash A : s_1^P$}
\AxiomC{$\Gamma \vdash B : \Diamond^* A$}
\AxiomC{$\Gamma \vdash C : \langle K[A] \rangle D$}
\TrinaryInfC{$\Gamma \vdash (K\, B) : \Diamond^* D$}
\end{prooftree}


\end{document}
